#!/bin/bash
#
# DataBridge Server Synchronization Script for simple projects
# Copyright (C) 2014-2015  Ioannis Charalampidis, PH-SFT, CERN

# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
#

# Base directory for the DataBridge
DB_URL="https://t4t-data-bridge.cern.ch"

# We will need the BOINC UserID and BOINC Authenticator as the first two parameters
AUTH_USER="$1"
AUTH_PASSWORD="$2"
[ -z "$AUTH_USER" ] && echo "ERROR: Please specify the authentication username!" && exit 2
[ -z "$AUTH_PASSWORD" ] && echo "ERROR: Please specify the authentication password!" && exit 2
[ ! -z "$3" ] && DB_URL="$3"

# Define URLs using base URL as reference
DB_INPUT_QUEUE="${DB_URL}/boinc-client/get-job.cgi"
DB_OUTPUT_QUEUE="${DB_URL}/boinc-client/put-jobout.cgi"
DB_INPUT_URL="${DB_URL}/myfed/t4t-boinc/input"
DB_OUTPUT_URL="${DB_URL}/myfed/t4t-boinc/output"

# Black Hole Protection: Minimum time required by a job (5 min)
BHP_MIN_JOB_TIME=300
# Black Hole Protection: Upper limit to reach when delaying next job
# request before declaring the job unprocessable.
BHP_MAX_DELAY=3600
# Black Hole Protection: The initial delay between consecutive job requests
BHP_INITIAL_DELAY=10

# DumbQ Metrics binary
DUMBQ_METRICS_BIN=$(which dumbq-metrics)

# Log helper
function log {
    # Echo log line with date
    local LEVEL=$1
    shift
    echo "[$(date '+%d/%m/%Y %H:%M:%S')] ${LEVEL}: $*" 1>&2
}

# DAVIX I/O Helper (file-based)
function davix_io {
    local VERB=$1
    local URL=$2
    local FILE=$3
    local RET=0
    local ERROR_MSG=""
    shift; shift; shift

    case $VERB in
        GET) 
            # Perform HTTP GET (davix-get also issues 'HEAD')
            ERROR_MSG=$(davix-http -k --userlogin ${AUTH_USER} --userpass ${AUTH_PASSWORD} $@ \
                -X GET -o ${FILE} "${URL}" 2>&1)
            RET=$?
            # Log Errors
            [ $RET -ne 0 ] && log "ERROR" "$ERROR_MSG"
            ;;
        PUT)
            # Perform HTTP PUT
            ERROR_MSG=$(davix-put -k --userlogin ${AUTH_USER} --userpass ${AUTH_PASSWORD} $@ \
                ${FILE} "${URL}" 2>&1)
            RET=$?
            # Log Errors
            [ $RET -ne 0 ] && log "ERROR" "$ERROR_MSG"
            ;;
        *)
            log "ERROR" "Unhandled HTTP Verb '${VERB}'"
            ;;
    esac

    # Return exit code
    return $RET
}

# CURL I/O Helper (payload-based)
function curl_io {
    local VERB=$1
    local URL=$2
    local RET=0
    local ERROR_MSG=""
    shift; shift

    # Perform HTTP GET
    exec 3>&1
    ERROR_MSG=$(curl -k -u "${AUTH_USER}:${AUTH_PASSWORD}" $@ \
        -f -s -S -L \
        --retry 2 --retry-delay 10 \
        -X ${VERB} "${URL}" 2>&1 1>&3)
    RET=$?
    exec 3>&-

    # Log Errors
    [ $RET -ne 0 ] && log "ERROR" "$ERROR_MSG"

    # Return exit code
    return $RET
}

function get_jobfile {
    local JOBDIR=$1

    # Get a job ID from the queue
    local NEXT_JOB_ID=$(curl_io GET "${DB_INPUT_QUEUE}" --header "userID: ${AUTH_USER}")
    if [ $? -ne 0 ]; then
        log "ERROR" "Could not fetch next job ID from queue"
        return 1
    fi

    # If queue is empty, return
    if [ -z "$NEXT_JOB_ID" ]; then
        log "WARN" "The job description got from the input queue is empty!"
        return 1
    fi

    # Download job file
    local JOB_FILE="${JOBDIR}/job.sh"
    davix_io GET "${DB_INPUT_URL}/${NEXT_JOB_ID}" "${JOB_FILE}"

    # If we got an error, return
    if [ $? -ne 0 ]; then
        log "ERROR" "Could not download job contents from server"
        return 1
    fi

    # Got file
    JOB_ID="$NEXT_JOB_ID"
    return 0
}

function upload_jobdir {
    local JOBDIR=$1
    local JOB_ID=$2
    local USER_DATA=$3
    local ERROR_MSG=""
    local UPLOAD_URL=""

    # Archive job directory
    local ARCHIVE_FILE="$(mktemp -u).tgz"
    ( cd ${JOBDIR}; tar -zcf ${ARCHIVE_FILE} ./* )

    # Upload archive directory
    UPLOAD_URL="${DB_OUTPUT_URL}/${JOB_ID}.tgz?userdata=${USER_DATA}"
    davix_io PUT "${UPLOAD_URL}" "${ARCHIVE_FILE}"
    if [ $? -ne 0 ]; then
        log "ERROR" "Unable to upload job to ${UPLOAD_URL}: ${ERROR_MSG}"
        return
    fi

    # Schedule the job to be picked-up by the output queue
    curl_io PUT "${DB_OUTPUT_QUEUE}" -F "userdata=${JOB_ID}" --header "userID: ${AUTH_USER}"
    if [ $? -ne 0 ]; then
        log "ERROR" "Unable to upload job description to ${DB_OUTPUT_QUEUE}: ${ERROR_MSG}"
        return
    fi

    # Remove archive file
    rm "${ARCHIVE_FILE}"
}

function cleanup {
    # Remove directory
    [ -d ${WORKDIR} -a ${#WORKDIR} -gt 1 ] && rm -rf ${WORKDIR}
    # Exit
    exit 0
}

# Trap cleanup
trap cleanup SIGINT

# Main program loop
BHP_TIMER=${BHP_INITIAL_DELAY}
while true; do

    # Create a temporary directory for the project
    WORKDIR=$(mktemp -d)

    # Update DumbQ metrics
    [ ! -z "$DUMBQ_METRICS_BIN" ] && ${DUMBQ_METRICS_BIN} --set status=waiting

    # Download job file
    JOB_ID=""
    while [ -z "$JOB_ID" ]; do

        # Log attempts
        log "INFO" "Fetching next job in queue"

        # Get next job file (updates JOB_ID)
        get_jobfile "${WORKDIR}"

        # Sleep on errors
        if [ $? -ne 0 ]; then
            log "INFO" "Sleeping for 1 minute"
            sleep 60
        fi

    done


    # Get the timestamp when the job has started
    TS_STARTED=$(date +%s)

    # Update DumbQ metrics
    [ ! -z "$DUMBQ_METRICS_BIN" ] && ${DUMBQ_METRICS_BIN} --set status=running

    # Run job
    log "INFO" "Starting job ${JOB_ID}"
    ( cd "${WORKDIR}"; chmod +x job.sh; exec ./job.sh ) >${WORKDIR}/job.stdout 2>${WORKDIR}/job.stderr

    # Get exit code
    EXIT_CODE=$?

    # Get the timestamp when the job has finished
    TS_FINISHED=$(date +%s)
    let TS_DELTA=${TS_FINISHED}-${TS_STARTED}

    # Upload results
    log "INFO" "Uploading results"
    upload_jobdir "${WORKDIR}" "${JOB_ID}" "exitcode=$EXIT_CODE&vmid=${DUMBQ_VMID}"

    # Cleanup
    log "INFO" "Cleaning-up workdir"
    rm -rf "${WORKDIR}"

    # Update DumbQ Metrics if existing
    if [ ! -z "$DUMBQ_METRICS_BIN" ]; then
        # Increment job metrics
        ${DUMBQ_METRICS_BIN} --add jobs=1 --add jobtime=${TS_DELTA} --avg jobaverage=${TS_DELTA} --set status=completed --set exitcode=${EXIT_CODE}
        # Update job status metrics
        if [ $EXIT_CODE -eq 0 ]; then
            ${DUMBQ_METRICS_BIN} --add completed=1
        else
            ${DUMBQ_METRICS_BIN} --add failed=1
        fi
    fi

    # Apply blackhole protection
    if [ $TS_DELTA -lt ${BHP_MIN_JOB_TIME} ]; then
        # The job is quitting faster than expected
        log "WARN" "Black hole detected. Job exited after ${TS_DELTA} seconds (<${BHP_MIN_JOB_TIME})"
        # Scale-up the blackhole timer delay
        let BHP_TIMER*=2
        # If we have exceeded BHP_MAX_DELAY, abort!
        if [ $BHP_TIMER -gt ${BHP_MAX_DELAY} ]; then
            log "ERROR" "Exceeded BHP max timer, will reboot!"
            reboot
            exit 1
        fi
    else
        # Otherwise the job works fine, keep initial delay
        BHP_TIMER=${BHP_INITIAL_DELAY}
    fi

    # Sleep a bit
    log "INFO" "Sleeping for ${BHP_TIMER} seconds"
    sleep ${BHP_TIMER}

done
