#!/bin/bash
#
# DataBridge Server Synchronization Script for simple projects
# Copyright (C) 2014-2015  Ioannis Charalampidis, PH-SFT, CERN

# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
#

# Base directory for the DataBridge
DB_URL="https://t4t-data-bridge.cern.ch"

# We will need the BOINC UserID and BOINC Authenticator as the first two parameters
AUTH_USER="$1"
AUTH_PASSWORD="$2"
[ -z "$AUTH_USER" ] && echo "ERROR: Please specify the authentication username!" && exit 2
[ -z "$AUTH_PASSWORD" ] && echo "ERROR: Please specify the authentication password!" && exit 2
[ ! -z "$3" ] && DB_URL="$3"

# Define URLs using base URL as reference
DB_QUEUE_URL="${DB_URL}/boinc-client/get-job.cgi"
DB_INPUT_URL="${DB_URL}/myfed/t4t-boinc/input"
DB_OUTPUT_URL="${DB_URL}/myfed/t4t-boinc/output"

# Black Hole Protection: Minimum time required by a job (5 min)
BHP_MIN_JOB_TIME=300
# Black Hole Protection: Upper limit to reach when delaying next job
# request before declaring the job unprocessable.
BHP_MAX_DELAY=3600
# Black Hole Protection: The initial delay between consecutive job requests
BHP_INITIAL_DELAY=5

# DumbQ Metrics binary
DUMBQ_METRICS_BIN=$(which dumbq-metrics)

function get_jobfile {
    local JOBDIR=$1

    # Get a job ID from the queue
    local NEXT_JOB_ID=$(curl -f -u "${AUTH_USER}:${AUTH_PASSWORD}" --header "userID: ${AUTH_USER}" -k -s "${DB_QUEUE_URL}" 2>/dev/null)

    # If we got an error, return
    if [ $? -ne 0 ]; then
        echo "ERROR: Could not fetch next job ID from queue"
        return 1
    fi

    # If queue is empty, return
    [ -z "$NEXT_JOB_ID" ] && return 1

    # Download job file
    local JOB_FILE="${JOBDIR}/job.sh"
    curl -f -o ${JOB_FILE} -u "${AUTH_USER}:${AUTH_PASSWORD}" -k -L -s "${DB_INPUT_URL}/${NEXT_JOB_ID}" 2>/dev/null

    # If we got an error, return
    if [ $? -ne 0 ]; then
        echo "ERROR: Could not download job contents from server"
        return 1
    fi

    # Got file
    JOB_ID="$NEXT_JOB_ID"
    return 0
}

function upload_jobdir {
    local JOBDIR=$1
    local JOB_ID=$2
    local USER_DATA=$3

    # Archive job directory
    local ARCHIVE_FILE="$(mktemp -u).tgz"
    ( cd ${JOBDIR}; tar -zcf ${ARCHIVE_FILE} ./* )

    # Upload archive directory
    curl -X PUT --upload "${ARCHIVE_FILE}" -u "${AUTH_USER}:${AUTH_PASSWORD}" -k -L -s "${DB_OUTPUT_URL}/${JOB_ID}.tgz?userdata=${USER_DATA}" 2>/dev/null

    # Remove archive file
    rm "${ARCHIVE_FILE}"
}

function cleanup {
    # Remove directory
    [ -d ${WORKDIR} -a ${#WORKDIR} -gt 1 ] && rm -rf ${WORKDIR}
    # Exit
    exit 0
}

# Trap cleanup
trap cleanup SIGINT

# Main program loop
BHP_TIMER=${BHP_INITIAL_DELAY}
while true; do

    # Create a temporary directory for the project
    WORKDIR=$(mktemp -d)

    # Update DumbQ metrics
    [ ! -z "$DUMBQ_METRICS_BIN" ] && ${DUMBQ_METRICS_BIN} --set status=waiting

    # Download job file
    JOB_ID=""
    while [ -z "$JOB_ID" ]; do

        # Log attempts
        echo "INFO: Fetching next job in queue"

        # Get next job file (updates JOB_ID)
        get_jobfile "${WORKDIR}"

        # Sleep on errors
        if [ $? -ne 0 ]; then
            echo "INFO: Sleeping for 1 minute"
            sleep 60
        fi

    done


    # Get the timestamp when the job has started
    TS_STARTED=$(date +%s)

    # Update DumbQ metrics
    [ ! -z "$DUMBQ_METRICS_BIN" ] && ${DUMBQ_METRICS_BIN} --set status=running

    # Run job
    echo "INFO: Starting job ${JOB_ID}"
    ( cd "${WORKDIR}"; chmod +x job.sh; exec ./job.sh ) >${WORKDIR}/job.stdout 2>${WORKDIR}/job.stderr

    # Get exit code
    EXIT_CODE=$?

    # Get the timestamp when the job has finished
    TS_FINISHED=$(date +%s)
    let TS_DELTA=${TS_FINISHED}-${TS_STARTED}

    # Upload results
    echo "INFO: Uploading results"
    upload_jobdir "${WORKDIR}" "${JOB_ID}" "exitcode=$EXIT_CODE&vmid=${DUMBQ_VMID}"

    # Cleanup
    echo "INFO: Cleaning-up workdir"
    rm -rf "${WORKDIR}"

    # Update DumbQ Metrics if existing
    if [ ! -z "$DUMBQ_METRICS_BIN" ]; then
        # Increment job metrics
        ${DUMBQ_METRICS_BIN} --add jobs=1 --avg jobtime=${TS_DELTA} --set status=completed --set exitcode=${EXIT_CODE}
        # Update job status metrics
        if [ $EXIT_CODE -eq 0 ]; then
            ${DUMBQ_METRICS_BIN} --add completed=1
        else
            ${DUMBQ_METRICS_BIN} --add failed=1
        fi
    fi

    # Apply blackhole protection
    if [ $TS_DELTA -lt ${BHP_MIN_JOB_TIME} ]; then
        # The job is quitting faster than expected
        echo "WARN: Job exited after ${TS_DELTA} seconds (> ${BHP_MIN_JOB_TIME})"
        # Scale-up the blackhole timer delay
        let BHP_TIMER*=2
        # If we have exceeded BHP_MAX_DELAY, abort!
        if [ $BHP_TIMER -gt ${BHP_MAX_DELAY} ]; then
            echo "ERROR: Exceeded BHP max timer will reboot!"
            reboot
            exit
        fi
    else
        # Otherwise the job works fine, keep initial delay
        BHP_TIMER=${BHP_INITIAL_DELAY}
    fi

    # Sleep a bit
    echo "INFO: Sleeping for ${BHP_TIMER} seconds"
    sleep ${BHP_TIMER}

done
