#!/bin/bash
#
# DataBridge Server Synchronization Script for simple projects
# Copyright (C) 2014-2015  Ioannis Charalampidis, PH-SFT, CERN

# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
#

###############################################

# Base directory for the DataBridge
DB_DOMAIN="t4t-data-bridge.cern.ch"

# We will need the BOINC UserID and BOINC Authenticator as the first two parameters
AUTH_USER="$1"
AUTH_PASSWORD="$2"
shift; shift;
[ -z "$AUTH_USER" ] && echo "ERROR: Please specify the authentication username!" && exit 2
[ -z "$AUTH_PASSWORD" ] && echo "ERROR: Please specify the authentication password!" && exit 2
[ ! -z "$1" ] && DB_DOMAIN="$1" && shift

# ------------------------
#  Databridge URLs
# ------------------------

# Define URLs using base URL as reference
DB_INPUT_QUEUE="https://${DB_DOMAIN}/boinc-client/get-job.cgi"
DB_OUTPUT_QUEUE="https://${DB_DOMAIN}/boinc-client/put-jobout.cgi"
DB_INPUT_URL="davs://${DB_DOMAIN}/myfed/t4t-boinc/input"
DB_OUTPUT_URL="davs://${DB_DOMAIN}/myfed/t4t-boinc/output"

# ------------------------
#  Black Hole Protection
# ------------------------

# Black Hole Protection: Minimum time required by a job (5 min)
BHP_MIN_JOB_TIME=300
# Black Hole Protection: Upper limit to reach when delaying next job
# request before declaring the job unprocessable.
BHP_MAX_DELAY=3600
# Black Hole Protection: The initial delay between consecutive job requests
BHP_INITIAL_DELAY=10

# --------------------------
#  CURL & DAVIX I/O Retries
# --------------------------

# How many times to re-try an I/O Command
IO_RETRIES=60

# How long to wait between I/O Retries
IO_RETRY_DELAY=60

###############################################

# DumbQ Metrics binary
DUMBQ_METRICS_BIN=$(which dumbq-metrics)

# Handle other command-line arguments
LOG_LEVEL=1
for I in $@; do
    case $I in
        --debug)
            # Enable debug logging
            LOG_LEVEL=0
            ;;
        *)
            echo "ERROR: Unknown argument '$I'" >&2
            ;;
    esac
done

# Log helper
function log {
    local LEVEL=$1
    local LEVEL_STR="DEBUG"
    shift

    # Do not log levels lower than
    # the ones specified
    [ $LEVEL -lt $LOG_LEVEL ] && return

    # Translate level to string
    if [ $LEVEL -eq 1 ]; then
        LEVEL_STR="INFO"
    elif [ $LEVEL -eq 2 ]; then
        LEVEL_STR="WARN"
    elif [ $LEVEL -eq 3 ]; then
        LEVEL_STR="ERROR"
    fi

    # Process multiline buffers
    while read L; do
        echo "[$(date '+%d/%m/%Y %H:%M:%S')] ${LEVEL_STR}: $L" 1>&2
    done <<< "$*"
}

# DAVIX I/O Helper (file-based)
function davix_io {
    local VERB=$1
    local URL=$2
    local FILE=$3
    local RET=256
    local ERROR_MSG=""
    local RETRIES=${IO_RETRIES}
    shift; shift; shift

    # Extra args
    local CMDLINE="-k --userlogin \"${AUTH_USER}\" --userpass \"${AUTH_PASSWORD}\""
    for (( i = 1; i <= $# ; i++ )); do
        eval ARG=\$$i
        if [[ $ARG =~ $WHITESPACE ]]
        then
            CMDLINE="$CMDLINE \"$ARG\""
        else
            CMDLINE="$CMDLINE $ARG"
        fi
    done

    # Handle verb accordint to case
    case $VERB in
        GET)
            # Use davix-get to download a file
            CMDLINE="davix-get $CMDLINE \"${URL}\" ${FILE}"
            ;;
        PUT)
            # Use davix-put to upload a file
            CMDLINE="davix-put $CMDLINE ${FILE} \"${URL}\""
            ;;
        DELETE)
            # Use davix-rm to delete a file
            CMDLINE="davix-rm $CMDLINE \"${URL}\""
            ;;
        *)
            log 3 "Unhandled DAVIX Verb '${VERB}'"
            return 200
            ;;
    esac

    # Repeat command if it fails
    while true; do

        # Perform DAVOX I/O
        log 0 "Executing $CMDLINE"
        exec 3>&1
        ERROR_MSG=$(eval $CMDLINE 2>&1)
        RET=$?
        exec 3>&-
        log 0 "Exited with $RET"

        # If this was an error, retry
        if [[ $RET -ne 0 && $RETRIES -gt 0 ]]; then
            let RETRIES--
            log 3 "$ERROR_MSG"
            log 1 "In ${IO_RETRY_DELAY} sec will retry a DAVIX ${VERB} to ${URL}"
            sleep ${IO_RETRY_DELAY}
        else
            break
        fi

    done

    # Log Errors
    [ $RET -ne 0 ] && log 3 "$ERROR_MSG"

    # Return exit code
    return $RET
}

# CURL I/O Helper (payload-based)
function curl_io {
    local VERB=$1
    local URL=$2
    local RET=0
    local ERROR_MSG=""
    local WHITESPACE="[[:space:]]"
    shift; shift

    # Extra args
    local CMDLINE="curl -k -u \"${AUTH_USER}:${AUTH_PASSWORD}\""
    CMDLINE="${CMDLINE} -f -s -S -L --retry ${IO_RETRIES} --retry-delay ${IO_RETRY_DELAY} -X ${VERB}"
    for (( i = 1; i <= $# ; i++ )); do
        eval ARG=\$$i
        if [[ $ARG =~ $WHITESPACE ]]
        then
            CMDLINE="$CMDLINE \"$ARG\""
        else
            CMDLINE="$CMDLINE $ARG"
        fi
    done
    CMDLINE="${CMDLINE} \"${URL}\""

    # Perform HTTP GET
    log 0 "Executing $CMDLINE"
    exec 3>&1
    ERROR_MSG=$(eval $CMDLINE 2>&1 1>&3)
    RET=$?
    exec 3>&-

    # Log Errors
    [ $RET -ne 0 ] && log 3 "$ERROR_MSG"

    # Return exit code
    return $RET
}

function get_jobfile {
    local JOBDIR=$1
    local NEXT_JOB_ID=""
    local TRIES=0

    # Get a job ID from the queue
    NEXT_JOB_ID=$(curl_io GET "${DB_INPUT_QUEUE}" --header "userID: ${AUTH_USER}")
    if [ $? -ne 0 ]; then
        log 3 "Could not fetch next job ID from queue"
        return 1
    fi

    # If queue is empty, return
    if [ -z "$NEXT_JOB_ID" ]; then
        log 2 "The job description got from the input queue is empty!"
        return 1
    fi

    # Retry multiple times to download job from queue
    while [ $TRIES ]

    # Download job file
    local JOB_FILE="${JOBDIR}/job.sh"
    davix_io GET "${DB_INPUT_URL}/${NEXT_JOB_ID}" "${JOB_FILE}"

    # If we got an error, return
    if [ $? -ne 0 ]; then
        log 3 "Could not download job contents from server"
        return 1
    fi

    # Got file
    JOB_ID="$NEXT_JOB_ID"
    return 0
}

function upload_jobdir {
    local JOBDIR=$1
    local JOB_ID=$2
    local USER_DATA=$3
    local UPLOAD_URL=""

    # Archive job directory
    local ARCHIVE_FILE="$(mktemp -u).tgz"
    ( cd ${JOBDIR}; tar -zcf ${ARCHIVE_FILE} ./* )

    # Upload archive directory
    UPLOAD_URL="${DB_OUTPUT_URL}/${JOB_ID}.tgz?userdata=${USER_DATA}"
    davix_io PUT "${UPLOAD_URL}" "${ARCHIVE_FILE}"
    if [ $? -ne 0 ]; then
        log 3 "Unable to upload job to ${UPLOAD_URL}"
        return
    fi

    # Schedule the job to be picked-up by the output queue
    curl_io PUT "${DB_OUTPUT_QUEUE}" -F "userdata=${JOB_ID}" --header "userID: ${AUTH_USER}"
    if [ $? -ne 0 ]; then
        log 3 "Unable to upload job description to ${DB_OUTPUT_QUEUE}"
        return
    fi

    # Remove archive file
    rm "${ARCHIVE_FILE}"
}

function cleanup {
    # Remove directory
    [ -d ${WORKDIR} -a ${#WORKDIR} -gt 1 ] && rm -rf ${WORKDIR}
    # Exit
    exit 0
}

# Trap cleanup
trap cleanup SIGINT

# Main program loop
BHP_TIMER=${BHP_INITIAL_DELAY}
while true; do

    # Create a temporary directory for the project
    WORKDIR=$(mktemp -d)

    # Update DumbQ metrics
    [ ! -z "$DUMBQ_METRICS_BIN" ] && ${DUMBQ_METRICS_BIN} --set status=waiting

    # Download job file
    JOB_ID=""
    while [ -z "$JOB_ID" ]; do

        # Log attempts
        log 1 "Fetching next job in queue"

        # Get next job file (updates JOB_ID)
        get_jobfile "${WORKDIR}"

        # Sleep on errors
        if [ $? -ne 0 ]; then
            log 1 "Sleeping for 1 minute"
            sleep 60
        fi

    done


    # Get the timestamp when the job has started
    TS_STARTED=$(date +%s)

    # Update DumbQ metrics
    [ ! -z "$DUMBQ_METRICS_BIN" ] && ${DUMBQ_METRICS_BIN} --set status=running

    # Run job
    log 1 "Starting job ${JOB_ID}"
    ( cd "${WORKDIR}"; chmod +x job.sh; exec ./job.sh ) >${WORKDIR}/job.stdout 2>${WORKDIR}/job.stderr

    # Get exit code
    EXIT_CODE=$?

    # Get the timestamp when the job has finished
    TS_FINISHED=$(date +%s)
    let TS_DELTA=${TS_FINISHED}-${TS_STARTED}

    # Upload results
    log 1 "Uploading results"
    upload_jobdir "${WORKDIR}" "${JOB_ID}" "exitcode=$EXIT_CODE&vmid=${DUMBQ_VMID}"

    # Cleanup
    log 1 "Cleaning-up workdir"
    rm -rf "${WORKDIR}"

    # Update DumbQ Metrics if existing
    if [ ! -z "$DUMBQ_METRICS_BIN" ]; then
        # Increment job metrics
        ${DUMBQ_METRICS_BIN} --add jobs=1 --add jobtime=${TS_DELTA} --avg jobaverage=${TS_DELTA} --set status=completed --set exitcode=${EXIT_CODE}
        # Update job status metrics
        if [ $EXIT_CODE -eq 0 ]; then
            ${DUMBQ_METRICS_BIN} --add completed=1
        else
            ${DUMBQ_METRICS_BIN} --add failed=1
        fi
    fi

    # Apply blackhole protection
    if [ $TS_DELTA -lt ${BHP_MIN_JOB_TIME} ]; then
        # The job is quitting faster than expected
        log 2 "Black hole detected. Job exited after ${TS_DELTA} seconds (<${BHP_MIN_JOB_TIME})"
        # Scale-up the blackhole timer delay
        let BHP_TIMER*=2
        # If we have exceeded BHP_MAX_DELAY, abort!
        if [ $BHP_TIMER -gt ${BHP_MAX_DELAY} ]; then
            log 3 "Exceeded BHP max timer, will reboot!"
            reboot
            exit 1
        fi
    else
        # Otherwise the job works fine, keep initial delay
        BHP_TIMER=${BHP_INITIAL_DELAY}
    fi

    # Sleep a bit
    log 1 "Sleeping for ${BHP_TIMER} seconds"
    sleep ${BHP_TIMER}

done
