#!/bin/bash
#
# DataBridge Server Synchronization Script for simple projects
# Copyright (C) 2014-2015  Ioannis Charalampidis, PH-SFT, CERN

# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
#

###############################################

# Base directory for the DataBridge
DB_DOMAIN="t4t-data-bridge.cern.ch"

# We will need the BOINC UserID and BOINC Authenticator as the first two parameters
AUTH_USER="$1"
AUTH_PASSWORD="$2"
shift; shift;
[ -z "$AUTH_USER" ] && echo "ERROR: Please specify the authentication username!" && exit 2
[ -z "$AUTH_PASSWORD" ] && echo "ERROR: Please specify the authentication password!" && exit 2
[ ! -z "$1" ] && DB_DOMAIN="$1" && shift

# ------------------------
# Davix config
# ------------------------

# Where to find davix binaries
DAVIX_VERSION="0.5.0"
DAVIX_ROOT="/cvmfs/sft.cern.ch/lcg/external/davix/${DAVIX_VERSION}/x86_64-slc6"
DAVIX_BIN="${DAVIX_ROOT}/usr/bin"

# Include davix libraries in environment
export LD_LIBRARY_PATH="${LD_LIBRARY_PATH}:${DAVIX_ROOT}/usr/lib64"

# ------------------------
#  Databridge URLs
# ------------------------

# Define URLs using base URL as reference
DB_INPUT_QUEUE="https://${DB_DOMAIN}/boinc-client/get-job.cgi"
DB_OUTPUT_QUEUE="https://${DB_DOMAIN}/boinc-client/put-jobout.cgi"
DB_INPUT_URL="davs://${DB_DOMAIN}/myfed/t4t-boinc/input"
DB_OUTPUT_URL="davs://${DB_DOMAIN}/myfed/t4t-boinc/output"

# ------------------------
#  Black Hole Protection
# ------------------------

# Black Hole Protection: Minimum time required by a job (5 min)
BHP_MIN_JOB_TIME=300
# Black Hole Protection: Upper limit to reach when delaying next job
# request before declaring the job unprocessable.
BHP_MAX_DELAY=3600
# Black Hole Protection: The initial delay between consecutive job requests
BHP_INITIAL_DELAY=10

# --------------------------
#  CURL & DAVIX I/O Retries
# --------------------------

# How many times to re-try an I/O Command
IO_RETRIES=10

# How long to wait between I/O Retries
IO_RETRY_DELAY=30

# --------------------------
#  Network Heartbeat
# --------------------------

# A list of IP addresses to ping before assuming network is down
HEARTBEAT_IPS="188.184.9.234 8.8.8.8"

# How many times to rely on heartbeat before quitting
HEARTBEAT_RETRIES=5

# How long to wait after considering network inaccessible
HEARTBEAT_DELAY=60

###############################################

# DumbQ Metrics binary
DUMBQ_METRICS_BIN=$(which dumbq-metrics)
DUMBQ_LOGERROR_BIN=$(which dumbq-logerror)

# Handle other command-line arguments
LOG_LEVEL=1
for I in $@; do
    case $I in
        --debug)
            # Enable debug logging
            LOG_LEVEL=0
            ;;
        *)
            echo "ERROR: Unknown argument '$I'" >&2
            ;;
    esac
done

# Log helper
function log {
    local LEVEL=$1
    local LEVEL_STR="DEBUG"
    shift

    # Do not log levels lower than
    # the ones specified
    [ $LEVEL -lt $LOG_LEVEL ] && return

    # Translate level to string
    if [ $LEVEL -eq 1 ]; then
        LEVEL_STR="INFO"
    elif [ $LEVEL -eq 2 ]; then
        LEVEL_STR="WARN"
    elif [ $LEVEL -eq 3 ]; then
        LEVEL_STR="ERROR"
    fi

    # Process multiline buffers
    while read L; do

        # Log error
        echo "[$(date '+%d/%m/%Y %H:%M:%S')] ${LEVEL_STR}: $L" 1>&2

        # Warnings and errors are also logged in dumbq
        if [[ ! -z "$DUMBQ_LOGERROR_BIN" && $LEVEL -ge 2 ]]; then
            ${DUMBQ_LOGERROR_BIN} --level $LEVEL "$L"
        fi

    done <<< "$*"
}

# DAVIX I/O Helper (file-based)
function davix_io {
    local VERB=$1
    local URL=$2
    local FILE=$3
    local RET=256
    local ERROR_MSG=""
    local RETRIES=${IO_RETRIES}
    shift; shift; shift

    # Extra args
    local CMDLINE="--retry 1 -k --userlogin \"${AUTH_USER}\" --userpass \"${AUTH_PASSWORD}\""
    for (( i = 1; i <= $# ; i++ )); do
        eval ARG=\$$i
        if [[ $ARG =~ $WHITESPACE ]]
        then
            CMDLINE="$CMDLINE \"$ARG\""
        else
            CMDLINE="$CMDLINE $ARG"
        fi
    done

    # Handle verb accordint to case
    case $VERB in
        GET)
            # Use davix-get to download a file
            CMDLINE="${DAVIX_BIN}/davix-get $CMDLINE \"${URL}\" ${FILE}"
            ;;
        PUT)
            # Use davix-put to upload a file
            CMDLINE="${DAVIX_BIN}/davix-put $CMDLINE ${FILE} \"${URL}\""
            ;;
        DELETE)
            # Use davix-rm to delete a file
            CMDLINE="${DAVIX_BIN}/davix-rm  $CMDLINE \"${URL}\""
            ;;
        *)
            log 3 "Unhandled DAVIX Verb '${VERB}'"
            return 200
            ;;
    esac

    # Repeat command if it fails
    while true; do

        # Perform DAVOX I/O
        log 0 "Executing $CMDLINE"
        exec 3>&1
        ERROR_MSG=$(eval $CMDLINE 2>&1)
        RET=$?
        exec 3>&-
        log 0 "Exited with $RET"

        # If this was an error, retry
        if [[ $RET -ne 0 ]]; then
            if [[ $RETRIES -gt 0 ]]; then

                # Check if that's actually a 404 error
                if [[ $ERROR_MSG == *"401"* ]]; then
                    return 241
                elif [[ $ERROR_MSG == *"403"* ]]; then
                    return 243
                elif [[ $ERROR_MSG == *"404"* ]]; then
                    return 244
                fi

                # Otherwise retry
                let RETRIES--
                log 3 "$ERROR_MSG"
                log 1 "In ${IO_RETRY_DELAY} sec will retry a DAVIX ${VERB} to ${URL}"
                sleep ${IO_RETRY_DELAY}

            else
                # Critical error: Exhausted retries!
                return 255
            fi
        else
            break
        fi

    done

    # Log Errors
    [ $RET -ne 0 ] && log 3 "$ERROR_MSG"

    # Return exit code
    return $RET
}

# CURL I/O Helper (payload-based)
function curl_io {
    local VERB=$1
    local URL=$2
    local RET=0
    local ERROR_MSG=""
    local WHITESPACE="[[:space:]]"
    local RETRIES=${IO_RETRIES}
    shift; shift

    # Extra args
    local CMDLINE="curl -k -u \"${AUTH_USER}:${AUTH_PASSWORD}\""
    CMDLINE="${CMDLINE} -f -s -S -L -X ${VERB}"
    for (( i = 1; i <= $# ; i++ )); do
        eval ARG=\$$i
        if [[ $ARG =~ $WHITESPACE ]]
        then
            CMDLINE="$CMDLINE \"$ARG\""
        else
            CMDLINE="$CMDLINE $ARG"
        fi
    done
    CMDLINE="${CMDLINE} \"${URL}\""

    # Repeat command if it fails
    while true; do

        # Perform HTTP GET
        log 0 "Executing $CMDLINE"
        exec 3>&1
        ERROR_MSG=$(eval $CMDLINE 2>&1 1>&3)
        RET=$?
        exec 3>&-

        # If this was an error, retry
        if [[ $RET -ne 0 ]]; then

            if [[ $RETRIES -gt 0 ]]; then
                
                # Check if that's actually a 404 error
                if [[ $ERROR_MSG == *"401"* ]]; then
                    return 241
                elif [[ $ERROR_MSG == *"403"* ]]; then
                    return 243
                elif [[ $ERROR_MSG == *"404"* ]]; then
                    return 244
                fi

                # Otherwise retry
                let RETRIES--
                log 3 "$ERROR_MSG"
                log 1 "In ${IO_RETRY_DELAY} sec will retry a CURL ${VERB} to ${URL}"
                sleep ${IO_RETRY_DELAY}
                
            else
                # Critical error: Exhausted retries!
                return 255
            fi

        else
            break
        fi

    done

    # Return exit code
    return $RET
}

function get_jobfile {
    local JOBDIR=$1
    local NEXT_JOB_ID=""
    local RET=0

    # Get a job ID from the queue
    NEXT_JOB_ID=$(curl_io GET "${DB_INPUT_QUEUE}" --header "userID: ${AUTH_USER}")
    RET=$?

    # Cases of 404,401,403 errors are special (recoverable)
    if [ $RET -eq 244 ]; then
        log 2 "There are no more jobs in the queue"
        return 3
    elif [ $RET -eq 241 ]; then
        log 3 "The server refused our credentials"
        return 2
    elif [ $RET -eq 243 ]; then
        log 3 "The server denied access"
        return 2
    # Cases of any other HTTP error sounds like trouble 
    # CURLE_HTTP_RETURNED_ERROR (22)
    elif [ $RET -eq 22 ]; then
        log 3 "A server error occured"
        return 1
    # Retry timeout returns 255
    elif [ $RET -eq 255 ]; then
        log 3 "Giving up on contacting server"
        return 255
    elif [ $RET -ne 0 ]; then
        log 3 "Could not fetch next job ID from queue"
        return 1
    fi

    # If queue is empty, return
    if [ -z "$NEXT_JOB_ID" ]; then
        log 2 "The job description got from the input queue is empty!"
        return 1
    fi

    # Download job file
    local JOB_FILE="${JOBDIR}/job.sh"
    davix_io GET "${DB_INPUT_URL}/${NEXT_JOB_ID}" "${JOB_FILE}"
    RET=$?

    # Cases of 404,401,403 errors are special (recoverable)
    if [ $RET -eq 241 ]; then
        log 3 "The queue denied our credentials"
        return 2
    elif [ $RET -eq 243 ]; then
        log 3 "The queue denied access"
        return 2
    elif [ $RET -eq 244 ]; then
        log 3 "The queue file is gone"
        return 2
    # Retry timeout returns 255
    elif [ $RET -eq 255 ]; then
        log 3 "Giving up on contacting server"
        return 255
    elif [ $RET -ne 0 ]; then
        log 3 "Could not download job contents from server"
        return 1
    fi

    # Got file
    JOB_ID="$NEXT_JOB_ID"
    return 0
}

function upload_jobdir {
    local JOBDIR=$1
    local JOB_ID=$2
    local USER_DATA=$3
    local UPLOAD_URL=""
    local RET=0

    # Archive job directory
    local ARCHIVE_FILE="$(mktemp -u).tgz"
    ( cd ${JOBDIR}; tar -zcf ${ARCHIVE_FILE} ./* )

    # Upload archive directory
    UPLOAD_URL="${DB_OUTPUT_URL}/${JOB_ID}.tgz?userdata=${USER_DATA}"
    davix_io PUT "${UPLOAD_URL}" "${ARCHIVE_FILE}"
    RET=$?

    # Handle errors
    if [ $RET -eq 255 ]; then
        log 3 "Giving up on contacting server"
        return 255
    elif [ $RET -ne 0 ]; then
        log 3 "Unable to upload job to ${UPLOAD_URL}"
        return 1
    fi

    # Schedule the job to be picked-up by the output queue
    curl_io PUT "${DB_OUTPUT_QUEUE}" -F "userdata=${JOB_ID}" --header "userID: ${AUTH_USER}"
    RET=$?

    # Handle errors
    if [ $RET -eq 255 ]; then
        log 3 "Giving up on contacting server"
        return 255
    elif [ $RET -eq 241 ]; then
        log 3 "The server refused our credentials"
        return 1
    elif [ $RET -eq 22 ]; then
        log 3 "A server error occured"
        return 1
    elif [ $RET -ne 0 ]; then
        log 3 "Unable to upload job description to ${DB_OUTPUT_QUEUE}"
        return 1
    fi

    # Remove archive file
    rm "${ARCHIVE_FILE}"

    # We are good
    return 0
}

# Test if network is working by sending ping
# probes to a list of heartbeat IPs
function network_heartbeat {
    # Ping heartbeat IPs
    for IP in $HEARTBEAT_IPS; do
        # Ping the IP address
        ping -c1 $IP 1>/dev/null 2>&1
        # If IP is alive we are good
        [ $? -eq 0 ] && return 0
    done
    # No IP was alive
    return 1
}

# Ensure that we have network
function network_validate {
    local ETH="eth0"
    local RETRIES=${HEARTBEAT_RETRIES}

    # Send a heartbeat probe
    network_heartbeat
    if [ $? -eq 0 ]; then
        # Exit
        return 0
    fi

    # If we did not have network, check if this
    # is caused by misconfigured network
    log 2 "Network looks inaccessible. Trying to re-configure."
    /sbin/ifdown ${ETH}
    /sbin/ifup ${ETH}

    # Check if it becomes available after a while
    while [ $RETRIES -gt 0 ]; do

        # Send a heartbeat probe
        network_heartbeat
        if [ $? -eq 0 ]; then
            log 1 "Network recovered. Resuming operations."
            return 0
        fi

        # Wait for a while
        sleep $HEARTBEAT_DELAY

        # Decrement counter
        let RETRIES--

    done

    # We didn't manage to solve anything
    return 1
}

function cleanup {
    # Remove directory
    [ -d ${WORKDIR} -a ${#WORKDIR} -gt 1 ] && rm -rf ${WORKDIR}
    # Exit
    exit 0
}

# Trap cleanup
trap cleanup SIGINT

# Main program loop
BHP_TIMER=${BHP_INITIAL_DELAY}
while true; do

    # Create a temporary directory for the project
    WORKDIR=$(mktemp -d)

    # Update DumbQ metrics
    [ ! -z "$DUMBQ_METRICS_BIN" ] && ${DUMBQ_METRICS_BIN} --set status=waiting

    # Download job file
    JOB_ID=""
    while [ -z "$JOB_ID" ]; do

        # Log attempts
        log 1 "Fetching next job in queue"

        # Make sure we have network
        network_validate
        if [ $? -eq 1 ]; then
            log 3 "Network is unaccessible, will exit"
            exit 3
        fi

        # Get next job file (updates JOB_ID)
        get_jobfile "${WORKDIR}"
        RET=$?

        # Exit on critical errors
        if [ $RET -eq 255 ]; then
            log 3 "Unrecoverable error occured, will exit"
            exit 3
        elif [ $RET -eq 2 ]; then
            log 1 "A recoverable error occured, will retry in a second"
            sleep 1
        elif [ $RET -ne 0 ]; then
            SLEEP_TIME=60
            let SLEEP_TIME+=$((RANDOM%30))
            log 1 "Sleeping for ${SLEEP_TIME} seconds"
            sleep $SLEEP_TIME
        fi

    done


    # Get the timestamp when the job has started
    TS_STARTED=$(date +%s)

    # Update DumbQ metrics
    [ ! -z "$DUMBQ_METRICS_BIN" ] && ${DUMBQ_METRICS_BIN} --set status=running

    # Run job
    log 1 "Starting job ${JOB_ID}"
    ( cd "${WORKDIR}"; chmod +x job.sh; exec ./job.sh ) >${WORKDIR}/job.stdout 2>${WORKDIR}/job.stderr

    # Get exit code
    EXIT_CODE=$?

    # Get the timestamp when the job has finished
    TS_FINISHED=$(date +%s)
    let TS_DELTA=${TS_FINISHED}-${TS_STARTED}

    # Make sure we have network
    network_validate
    if [ $? -eq 1 ]; then
        log 2 "Network is unaccessible, but not giving up until we manage to upload output"
    fi

    # Upload results
    log 1 "Uploading results"
    upload_jobdir "${WORKDIR}" "${JOB_ID}" "exitcode=$EXIT_CODE&vmid=${DUMBQ_VMID}"
    if [ $? -eq 255 ]; then
        log 3 "Unrecoverable error occured, will exit"
        exit 3
    fi

    # Cleanup
    log 1 "Cleaning-up workdir"
    rm -rf "${WORKDIR}"

    # Update DumbQ Metrics if existing
    if [ ! -z "$DUMBQ_METRICS_BIN" ]; then
        # Increment job metrics
        ${DUMBQ_METRICS_BIN} --add jobs=1 --add jobtime=${TS_DELTA} --avg jobaverage=${TS_DELTA} --set status=completed --set exitcode=${EXIT_CODE}
        # Update job status metrics
        if [ $EXIT_CODE -eq 0 ]; then
            ${DUMBQ_METRICS_BIN} --add completed=1
        else
            ${DUMBQ_METRICS_BIN} --add failed=1
        fi
    fi

    # Apply blackhole protection
    if [ $TS_DELTA -lt ${BHP_MIN_JOB_TIME} ]; then
        # The job is quitting faster than expected
        log 2 "Black hole detected. Job exited after ${TS_DELTA} seconds (<${BHP_MIN_JOB_TIME})"
        # Scale-up the blackhole timer delay
        let BHP_TIMER*=2
        # If we have exceeded BHP_MAX_DELAY, abort!
        if [ $BHP_TIMER -gt ${BHP_MAX_DELAY} ]; then
            log 3 "It looks like we are consuming too many jobs, will exit"
            reboot
            exit 3
        fi
    else
        # Otherwise the job works fine, keep initial delay
        BHP_TIMER=${BHP_INITIAL_DELAY}
    fi

    # Sleep a bit
    log 1 "Sleeping for ${BHP_TIMER} seconds"
    sleep ${BHP_TIMER}

done
