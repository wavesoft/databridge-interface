#!/bin/bash
#
# DataBridge Server Synchronization Script for simple projects
# Copyright (C) 2014-2015  Ioannis Charalampidis, PH-SFT, CERN

# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
#

# Where the server configuration of DataBridge server-sync is located
CONFIG_FILE="/etc/databridge/server.conf"

function log {
	local LEVEL=$1
	local LEVEL_STR="DEBUG"
	shift

	# Translate level to string
	if [ $LEVEL -eq 1 ]; then
		LEVEL_STR="INFO"
	elif [ $LEVEL -eq 2 ]; then
		LEVEL_STR="WARN"
	elif [ $LEVEL -eq 3 ]; then
		LEVEL_STR="ERROR"
	fi

	# Check for log destination
	if [ -z "${DATABRIDGE_LOG_FILE}" ]; then
		echo "${LEVEL_STR}: $*"
	else
		echo "$(date -R) ${LEVEL_STR}: $*" >> ${DATABRIDGE_LOG_FILE}
	fi
}

function put_jobfile {
	local FILE=$1

	# Validate file
	[ ! -f ${FILE} ] && log 3 "File $FILE does not exist" && return 1

	# Get file UUID
	local JOB_ID=$(uuidgen)	

	# Upload file on input queue
	log 1 "Uploading job file ${FILE}"
	curl -f -X PUT --upload ${FILE} --key ${DATABRIDGE_SSL_KEY} --cert ${DATABRIDGE_SSL_CERT} -k -L -s "${DATABRIDGE_INPUT_URL}/${JOB_ID}"
	[ $? -ne 0 ] && return 1
	
	# Upload job ID on FIFO	
	log 1 "Uploading job description"
	curl -f -X PUT -F "userdata=${JOB_ID}" --header "jobID: ${JOB_ID}" --key ${DATABRIDGE_SSL_KEY} --cert ${DATABRIDGE_SSL_CERT} -k -s "${DATABRIDGE_QUEUE_URL}"
	if [ $? -ne 0 ]; then
		# Remove from queue
		log 3 "Could not upload job description, removing job file"
		curl -f -X DELETE --key ${DATABRIDGE_SSL_KEY} --cert ${DATABRIDGE_SSL_CERT} -k -L -s "${DATABRIDGE_INPUT_URL}/${JOB_ID}"
		# Return error
		return 1
	fi
	
	# Successfuly placed, update state file
	log 1 "Job ${JOB_ID} placed in queue"
	echo "$JOB_ID:$(date +%s)" >> $DATABRIDGE_JOB_LIST

	# Return 0
	return 0
}

function get_outfile {
	local JOB_ID=$1
	local F_JOB=""
	local F_HEADERS=""
	local H_LAST_MODIFIED=""
	local JOB_DATE=""
	local JOB_OUTPUT_DIR=""

	# Temporary files
	F_JOB="${TMP_DIR}/jobout${DATABRIDGE_OUTPUT_EXT}"
	F_HEADERS="${TMP_DIR}/headers.txt"

	# Try to get job file from output directory into the temporary files
	curl -f -o "${F_JOB}" --key ${DATABRIDGE_SSL_KEY} --cert ${DATABRIDGE_SSL_CERT} -k -L -s -D "${F_HEADERS}" -f "${DATABRIDGE_OUTPUT_URL}/${JOB_ID}.tgz"
	[ $? -ne 0 ] && return 1

	# Try to get Last-Modified header from response
	H_LAST_MODIFIED=$(cat ${F_HEADERS} | grep Last-Modified | awk -F': ' '{print $2}')

	# Fallback to 'now' if we don't have a Last-Modified header
	if [ -z "${H_LAST_MODIFIED}" ]; then
		JOB_DATE=$(date +%Y%m%d)
	else
		JOB_DATE=$(date +%Y%m%d -d "${H_LAST_MODIFIED}")
	fi

	# Make sure directory exists
	JOB_OUTPUT_DIR="${DATABRIDGE_OUTPUT_DIR}/${JOB_DATE}"
	if [ ! -d "${JOB_OUTPUT_DIR}" ]; then

		# Create directory
		mkdir -p "${JOB_OUTPUT_DIR}" 2>/dev/null
		[ $? -ne 0 ] && log 3 "Could not create directory ${JOB_OUTPUT_DIR}" && return 1

		# Change owner if reuqired
		if [ ! -z "${DATABRIDGE_OUTPUT_OWNER}" ]; then
			chown ${DATABRIDGE_OUTPUT_OWNER} "${JOB_OUTPUT_DIR}" 2>/dev/null
			[ $? -ne 0 ] && log 2 "Could not change ownership of ${JOB_OUTPUT_DIR}"
		fi

	fi

	# Move file to the proper location
	mv "${F_JOB}" "${JOB_OUTPUT_DIR}/${JOB_ID}${DATABRIDGE_OUTPUT_EXT}" 2>/dev/null
	[ $? -ne 0 ] && log 3 "Could not move job file to ${JOB_OUTPUT_DIR}/${JOB_ID}${DATABRIDGE_OUTPUT_EXT}" && return 1

	# Change owner if reuqired
	if [ ! -z "${DATABRIDGE_OUTPUT_OWNER}" ]; then
		chown ${DATABRIDGE_OUTPUT_OWNER} "${JOB_OUTPUT_DIR}/${JOB_ID}${DATABRIDGE_OUTPUT_EXT}" 2>/dev/null
		[ $? -ne 0 ] && log 2 "Could not change ownership of ${JOB_OUTPUT_DIR}/${JOB_ID}${DATABRIDGE_OUTPUT_EXT}"
	fi

	# Everything worked as expected, delete output (cleanup)
	curl -f -X DELETE --key ${DATABRIDGE_SSL_KEY} --cert ${DATABRIDGE_SSL_CERT} -k -L -s "${DATABRIDGE_OUTPUT_URL}/${JOB_ID}.tgz"
	[ $? -ne 0 ] && log 2 "Unable to clean job output of job ${JOB_ID}"

	# Return 0
	return 0
}

function upload_jobs {
	local COUNTER=0
	local FILE=""

	# Start submitting jobs
	for FILE in ${DATABRIDGE_INPUT_DIR}/*; do

		# Put job file
		put_jobfile $FILE

		# Upon successful placement, remove
		if [ $? -eq 0 ]; then
			rm $FILE
		else
			log 3 "Could not upload file $FILE, will try later"
		fi

		# Increment counter
		let COUNTER+=1
		if [ $COUNTER -ge ${UPLOAD_LIMIT} ]; then
			log 1 "Reached limit of ${UPLOAD_LIMIT} jobs per submission"
			break
		fi

	done

}

function download_jobs {
	local STALE_COUNTER=0
	local JOB_ID=""
	local JOB_TS=""
	local JOB=""
	local ROTATE_FILE="${TMP_DIR}/joblist.rotate"

	# Prepare rotation file
	echo -n "" > ${ROTATE_FILE}

	# Start reading the state file
	while read JOB; do

		# Skip empty lines
		[ -z "$JOB" ] && continue

		# Drain staled state
		if [ $STALE_COUNTER -gt ${DATABRIDGE_DOWNLOAD_STALE_RATE} ]; then
			echo "$JOB" >> ${ROTATE_FILE}
			continue
		fi

		# Get job info
		JOB_ID=$(echo "$JOB" | awk -F':' '{print $1}')
		JOB_TS=$(echo "$JOB" | awk -F':' '{print $2}')

		# Try to get job file
		get_outfile ${JOB_ID}

		# If we didn't manage, re-schedule attempt later
		if [ $? -ne 0 ]; then

			# Check for expired job
			let DELTA=$(date +%s)-${JOB_TS}
			if [ ${DELTA} -gt ${DATABRIDGE_JOB_TIMEOUT} ]; then
				# Warn the user, and don't touch stale counter
				log 2 "Job ${JOB_ID} expired after ${DELTA} seconds"
			else
				# Otherwise process later
				echo "$JOB" >> ${ROTATE_FILE}
				# Increment stale counter
				let STALE_COUNTER+=1
				# Warn for stale
				[  $STALE_COUNTER -gt ${DATABRIDGE_DOWNLOAD_STALE_RATE} ] && log 2 "Too many input jobs without data. Reading staled!"
			fi

		else
			echo "INFO: Job ${JOB_ID} completed"
			# Reset stale counter
			STALE_COUNTER=0
		fi

	done < ${DATABRIDGE_JOB_LIST}

	# Rotate state file
	cp -f --no-preserve=mode,ownership ${ROTATE_FILE} ${DATABRIDGE_JOB_LIST}

}

# Check for override in the config file
[ ! -z "$1" ] && CONFIG_FILE=$1

# Lookup for the config
if [ ! -f ${CONFIG_FILE} ]; then
	log 3 "Could not find databridge server configuration in ${CONFIG_FILE}!"
	exit 1
fi

# Source config
. ${CONFIG_FILE}

# Make sure we have input/output directories
mkdir -p ${DATABRIDGE_INPUT_DIR}
mkdir -p ${DATABRIDGE_OUTPUT_DIR}
mkdir -p $(dirname ${DATABRIDGE_JOB_LIST})

# Create a temporary directory
TMP_DIR=$(mktemp -d)

# Start by downloading jobs
log 1 "Downloading job outputs from DataBridge"
download_jobs

# Then upload jobs
log 1 "Uploading new jobs to DataBridge"
upload_jobs

# Remove temporary directory
rm -rf "${TMP_DIR}"

# We are done
log 1 "DataBridge synchronization completed"
