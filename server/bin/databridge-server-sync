#!/bin/bash
#
# DataBridge Server Synchronization Script for simple projects
# Copyright (C) 2014-2015  Ioannis Charalampidis, PH-SFT, CERN

# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
#

# Where the server configuration of DataBridge server-sync is located
CONFIG_FILE="/etc/databridge/server.conf"

function put_jobfile {
	local FILE=$1

	# Validate file
	[ ! -f ${FILE} ] && echo "ERROR: File $FILE does not exist" && return 1

	# Get file UUID
	local JOB_ID=$(uuidgen)	

	# Upload file on input queue
	echo "INFO: Uploading job file ${FILE}"
	curl -f -X PUT --upload ${FILE} --key ${DATABRIDGE_SSL_KEY} --cert ${DATABRIDGE_SSL_CERT} -k -L -s "${DATABRIDGE_INPUT_URL}/${JOB_ID}"
	[ $? -ne 0 ] && return 1
	
	# Upload job ID on FIFO	
	echo "INFO: Uploading job description"
	curl -f -X PUT -F "userdata=${JOB_ID}" --header "jobID: ${JOB_ID}" --key ${DATABRIDGE_SSL_KEY} --cert ${DATABRIDGE_SSL_CERT} -k -s "${DATABRIDGE_QUEUE_URL}"
	if [ $? -ne 0 ]; then
		# Remove from queue
		echo "ERROR: Could not upload job description, removing job file"
		curl -f -X DELETE --key ${DATABRIDGE_SSL_KEY} --cert ${DATABRIDGE_SSL_CERT} -k -L -s "${DATABRIDGE_INPUT_URL}/${JOB_ID}"
		# Return error
		return 1
	fi
	
	# Successfuly placed, update state file
	echo "INFO: Job ${JOB_ID} placed in queue"
	echo "$JOB_ID:$(date +%s)" >> $DATABRIDGE_JOB_LIST

	# Return 0
	return 0
}

function get_outfile {
	local JOB_ID=$1
	local F_JOB=""
	local F_HEADERS=""
	local H_LAST_MODIFIED=""
	local JOB_DATE=""
	local JOB_OUTPUT_DIR=""

	# Temporary files
	F_JOB="${TMP_DIR}/jobout.tar.gz"
	F_HEADERS="${TMP_DIR}/headers.txt"

	# Try to get job file from output directory into the temporary files
	curl -f -o "${F_JOB}" --key ${DATABRIDGE_SSL_KEY} --cert ${DATABRIDGE_SSL_CERT} -k -L -s -D "${F_HEADERS}" -f "${DATABRIDGE_OUTPUT_URL}/${JOB_ID}.tgz"
	[ $? -ne 0 ] && return 1

	# Try to get Last-Modified header from response
	H_LAST_MODIFIED=$(cat ${F_HEADERS} | grep Last-Modified | awk -F': ' '{print $2}')

	# Fallback to 'now' if we don't have a Last-Modified header
	if [ -z "${H_LAST_MODIFIED}" ]; then
		JOB_DATE=$(date +%Y%m%d)
	else
		JOB_DATE=$(date +%Y%m%d -d "${H_LAST_MODIFIED}")
	fi

	# Make sure directory exists
	JOB_OUTPUT_DIR="${DATABRIDGE_OUTPUT_DIR}/${JOB_DATE}"
	if [ ! -d "${JOB_OUTPUT_DIR}" ]; then

		# Create directory
		mkdir -p "${JOB_OUTPUT_DIR}"

		# Change owner if reuqired
		[ ! -z "${DATABRIDGE_OUTPUT_OWNER}" ] && chown ${DATABRIDGE_OUTPUT_OWNER} "${JOB_OUTPUT_DIR}"

	fi

	# Move file to the proper location
	mv "${F_JOB}" "${JOB_OUTPUT_DIR}/${JOB_ID}.tar.gz"

	# Change owner if reuqired
	[ ! -z "${DATABRIDGE_OUTPUT_OWNER}" ] && chown ${DATABRIDGE_OUTPUT_OWNER} "${JOB_OUTPUT_DIR}/${JOB_ID}.tar.gz"

	# Everything worked as expected, delete output (cleanup)
	curl -f -X DELETE --key ${DATABRIDGE_SSL_KEY} --cert ${DATABRIDGE_SSL_CERT} -k -L -s "${DATABRIDGE_OUTPUT_URL}/${JOB_ID}.tgz"
	[ $? -ne 0 ] && echo "WARNING: Unable to clean job output of job ${JOB_ID}"

	# Return 0
	return 0
}

function upload_jobs {
	local COUNTER=0
	local FILE=""

	# Start submitting jobs
	for FILE in ${DATABRIDGE_INPUT_DIR}/*; do

		# Put job file
		put_jobfile $FILE

		# Upon successful placement, remove
		if [ $? -eq 0 ]; then
			rm $FILE
		else
			echo "ERROR: Could not upload file $FILE, will try later"
		fi

		# Increment counter
		let COUNTER+=1
		if [ $COUNTER -ge ${DATABRIDGE_UPLOAD_BULK_SIZE} ]; then
			echo "INFO: Reached limit of ${DATABRIDGE_UPLOAD_BULK_SIZE} jobs per submission"
			break
		fi

	done

}

function download_jobs {
	local STALE_COUNTER=0
	local JOB_ID=""
	local JOB_TS=""
	local JOB=""
	local ROTATE_FILE="${TMP_DIR}/joblist.rotate"

	# Prepare rotation file
	echo -n "" > ${ROTATE_FILE}

	# Start reading the state file
	while read JOB; do

		# Skip empty lines
		[ -z "$JOB" ] && continue

		# Drain staled state
		if [ $STALE_COUNTER -gt ${DATABRIDGE_DOWNLOAD_STALE_RATE} ]; then
			echo "$JOB" >> ${ROTATE_FILE}
			continue
		fi

		# Get job info
		JOB_ID=$(echo "$JOB" | awk -F':' '{print $1}')
		JOB_TS=$(echo "$JOB" | awk -F':' '{print $2}')

		# Check for expired
		let DELTA=$(date +%s)-${JOB_TS}
		if [ ${DELTA} -gt ${DATABRIDGE_JOB_TIMEOUT} ]; then
			echo "WARN: Job ${JOB_ID} expired after ${DELTA} seconds"
		else
			# Try to get job file
			get_outfile ${JOB_ID}
			# If we didn't manage, re-schedule attempt later
			if [ $? -ne 0 ]; then
				# Process later
				echo "$JOB" >> ${ROTATE_FILE}
				# Increment stale counter
				let STALE_COUNTER+=1
				# Warn for stale
				[  $STALE_COUNTER -gt ${DATABRIDGE_DOWNLOAD_STALE_RATE} ] && echo "WARN: Too many input jobs without data. Reading staled!"
			else
				echo "INFO: Job ${JOB_ID} completed"
				# Reset stale counter
				STALE_COUNTER=0
			fi
		fi

	done < ${DATABRIDGE_JOB_LIST}

	# Rotate state file
	cp -f --no-preserve=mode,ownership ${ROTATE_FILE} ${DATABRIDGE_JOB_LIST}

}

# Check for override in the config file
[ ! -z "$1" ] && CONFIG_FILE=$1

# Lookup for the config
if [ ! -f ${CONFIG_FILE} ]; then
	echo "ERROR: Could not find databridge server configuration in ${CONFIG_FILE}!"
	exit 1
fi

# Source config
. ${CONFIG_FILE}

# Make sure we have input/output directories
mkdir -p ${DATABRIDGE_INPUT_DIR}
mkdir -p ${DATABRIDGE_OUTPUT_DIR}
mkdir -p $(dirname ${DATABRIDGE_JOB_LIST})

# Create a temporary directory
TMP_DIR=$(mktemp -d)

# Start by downloading jobs
echo "INFO: Downloading job outputs from DataBridge"
download_jobs

# Then upload jobs
echo "INFO: Uploading new jobs to DataBridge"
upload_jobs

# Remove temporary directory
rm -rf "${TMP_DIR}"

# We are done
echo "INFO: DataBridge synchronization completed"
