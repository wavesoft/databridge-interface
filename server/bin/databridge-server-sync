#!/bin/bash
#
# DataBridge Server Synchronization Script for simple projects
# Copyright (C) 2014-2015  Ioannis Charalampidis, PH-SFT, CERN

# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
#

# Where the server configuration of DataBridge server-sync is located
CONFIG_FILE="/etc/databridge/server.conf"

function log {
	local LEVEL=$1
	local LEVEL_STR="DEBUG"
	shift

	# Translate level to string
	if [ $LEVEL -eq 1 ]; then
		LEVEL_STR="INFO"
	elif [ $LEVEL -eq 2 ]; then
		LEVEL_STR="WARN"
	elif [ $LEVEL -eq 3 ]; then
		LEVEL_STR="ERROR"
	fi

	# Check for log destination
	if [ -z "${DATABRIDGE_LOG_FILE}" ]; then
		echo "${LEVEL_STR}: $*"
	else
		echo "$(date -R) ${LEVEL_STR}: $*" >> ${DATABRIDGE_LOG_FILE}
	fi
}

function put_jobfile {
	local FILE=$1

	# Validate file
	[ ! -f ${FILE} ] && log 3 "File $FILE does not exist" && return 1

	# Get file UUID
	JOB_ID=$(uuidgen)	

	# Upload file on input queue
	log 1 "Uploading job file ${FILE}"
	curl -f -X PUT --upload ${FILE} --key ${DATABRIDGE_SSL_KEY} --cert ${DATABRIDGE_SSL_CERT} -k -L -s "${DATABRIDGE_INPUT_URL}/${JOB_ID}"
	[ $? -ne 0 ] && return 1
	
	# Upload job ID on FIFO	
	log 1 "Uploading job description"
	curl -f -X PUT -F "userdata=${JOB_ID}" --header "jobID: ${JOB_ID}" --key ${DATABRIDGE_SSL_KEY} --cert ${DATABRIDGE_SSL_CERT} -k -s "${DATABRIDGE_QUEUE_URL}"
	if [ $? -ne 0 ]; then
		# Remove from queue
		log 3 "Could not upload job description, removing job file"
		curl -f -X DELETE --key ${DATABRIDGE_SSL_KEY} --cert ${DATABRIDGE_SSL_CERT} -k -L -s "${DATABRIDGE_INPUT_URL}/${JOB_ID}"
		# Return error
		return 1
	fi
	
	# Successfuly placed, update state file
	log 1 "Job ${JOB_ID} placed in queue"
	echo "$JOB_ID:$(date +%s)" >> $DATABRIDGE_JOB_LIST

	# Fire appropriate callback
	if [ ! -z "${DATABRIDGE_CB_SCHEDULED}" ]; then
		# Fire callback
		${DATABRIDGE_CB_SCHEDULED} "${FILE}" "${JOB_ID}"
		ANS=$?
		# Check result
		if [ $? -ne 0 ]; then
			log 2 "SCHEDULED callback failed (exit code $ANS)"
		else
			log 1 "SCHEDULED callback executed successfully"
		fi
	fi

	# Return 0
	return 0
}

function get_outfile {
	local JOB_ID=$1
	local F_JOB=""
	local F_HEADERS=""
	local H_LAST_MODIFIED=""
	local JOB_DATE=""
	local JOB_OUTPUT_DIR=""

	# Temporary files
	F_JOB="${TMP_DIR}/jobout${DATABRIDGE_OUTPUT_EXT}"
	F_HEADERS="${TMP_DIR}/headers.txt"

	# Try to get job file from output directory into the temporary files
	log 1 "Downloading job ${JOB_ID} from server"
	curl -f -o "${F_JOB}" --key ${DATABRIDGE_SSL_KEY} --cert ${DATABRIDGE_SSL_CERT} -k -L -s -D "${F_HEADERS}" -f "${DATABRIDGE_OUTPUT_URL}/${JOB_ID}.tgz"
	[ $? -ne 0 ] && return 1

	# Try to get Last-Modified header from response
	H_LAST_MODIFIED=$(cat ${F_HEADERS} | grep Last-Modified | awk -F': ' '{print $2}')

	# Fallback to 'now' if we don't have a Last-Modified header
	if [ -z "${H_LAST_MODIFIED}" ]; then
		JOB_DATE=$(date +%Y%m%d)
	else
		JOB_DATE=$(date +%Y%m%d -d "${H_LAST_MODIFIED}")
	fi

	# Make sure directory exists
	JOB_OUTPUT_DIR="${DATABRIDGE_OUTPUT_DIR}/${JOB_DATE}"
	if [ ! -d "${JOB_OUTPUT_DIR}" ]; then

		# Create directory
		mkdir -p "${JOB_OUTPUT_DIR}" 2>/dev/null
		[ $? -ne 0 ] && log 3 "Could not create directory ${JOB_OUTPUT_DIR}" && return 1

		# Change owner if reuqired
		if [ ! -z "${DATABRIDGE_OUTPUT_OWNER}" ]; then
			chown ${DATABRIDGE_OUTPUT_OWNER} "${JOB_OUTPUT_DIR}" 2>/dev/null
			[ $? -ne 0 ] && log 2 "Could not change ownership of ${JOB_OUTPUT_DIR}"
		fi

	fi

	# Move file to the proper location
	JOB_OUTFILE="${JOB_OUTPUT_DIR}/${JOB_ID}${DATABRIDGE_OUTPUT_EXT}"
	mv "${F_JOB}" "${JOB_OUTFILE}" 2>/dev/null
	[ $? -ne 0 ] && log 3 "Could not move job file to ${JOB_OUTPUT_DIR}/${JOB_ID}${DATABRIDGE_OUTPUT_EXT}" && return 1

	# Change owner if reuqired
	if [ ! -z "${DATABRIDGE_OUTPUT_OWNER}" ]; then
		chown ${DATABRIDGE_OUTPUT_OWNER} "${JOB_OUTPUT_DIR}/${JOB_ID}${DATABRIDGE_OUTPUT_EXT}" 2>/dev/null
		[ $? -ne 0 ] && log 2 "Could not change ownership of ${JOB_OUTPUT_DIR}/${JOB_ID}${DATABRIDGE_OUTPUT_EXT}"
	fi

	# Everything worked as expected, delete output (cleanup)
	curl -f -X DELETE --key ${DATABRIDGE_SSL_KEY} --cert ${DATABRIDGE_SSL_CERT} -k -L -s "${DATABRIDGE_OUTPUT_URL}/${JOB_ID}.tgz"
	[ $? -ne 0 ] && log 2 "Unable to clean job output of job ${JOB_ID}"

	# Fire appropriate callback
	if [ ! -z "${DATABRIDGE_CB_COMPLETED}" ]; then
		# Fire callback
		${DATABRIDGE_CB_COMPLETED} "${JOB_OUTFILE}" "${JOB_ID}"
		ANS=$?
		# Check result
		if [ $? -ne 0 ]; then
			log 2 "COMPLETED callback failed (exit code $ANS)"
		else
			log 1 "COMPLETED callback executed successfully"
		fi
	fi

	# Return 0
	return 0
}

function upload_jobs {
	local COUNTER=0
	local FILE=""
	local QUEUE_SIZE=0
	local UPLOAD_LIMIT=${DATABRIDGE_BULK_SIZE}

	# Check if we should cap the UPLOAD_LIMIT
	if [ ${DATABRIDGE_MAX_QUEUE_SIZE} -gt 0 ]; then

		# Check the length of the queue
		QUEUE_SIZE=$(cat ${DATABRIDGE_JOB_LIST} | wc -l)

		# Check delta
		let UPLOAD_LIMIT=${DATABRIDGE_MAX_QUEUE_SIZE}-${QUEUE_SIZE}

		# If less than 0, don't bother
		if [ ${UPLOAD_LIMIT} -le 0 ]; then
			log 1 "Skipping because queue contains ${QUEUE_SIZE} jobs"
			return
		fi

	fi

	# Start submitting jobs
	for FILE in ${DATABRIDGE_INPUT_DIR}/*; do

		# Put job file
		put_jobfile $FILE

		# Upon successful placement, remove
		if [ $? -eq 0 ]; then

			# Delete file
			rm $FILE

		else
			log 3 "Could not upload file $FILE, will try later"
		fi

		# Increment counter
		let COUNTER+=1
		if [ $COUNTER -ge ${UPLOAD_LIMIT} ]; then
			log 1 "Reached limit of ${UPLOAD_LIMIT} jobs per submission"
			break
		fi

	done

}

function download_jobs_fifo {
	local DELTA=0
	local COUNTER=0
	local JOB=""
	local JOB_ID=""
	local JOB_TS=""
	local CHECK_ID=""
	local ROTATE_TMP="${TMP_DIR}/joblist.tmp"
	local ROTATE_FILE="${TMP_DIR}/joblist.rotate"

	# Copy the IDs of the jobs pending in the queue.
	# This will help us at a later time to track changes.
	cp -f ${DATABRIDGE_JOB_LIST} ${ROTATE_TMP}

	# Repeat for a while
	while true; do

		# Get next job from the output fifo
		JOB_ID=$(curl -L -f --key ${DATABRIDGE_SSL_KEY} --cert ${DATABRIDGE_SSL_CERT} -k -s "${DATABRIDGE_OUTQUEUE_URL}" 2>/dev/null)
		if [ -z "$JOB_ID" ]; then
			log 1 "No more jobs in the output queue"
			break
		fi

		# Try to get job file
		get_outfile ${JOB_ID}
		if [ $? -eq 0 ]; then
			log 1 "Job ${JOB_ID} completed"
		else
			log 1 "Job ${JOB_ID} could not be downloaded"
		fi

		# Keep the ID of the processed job
		echo "${JOB_ID}" >> ${ROTATE_TMP}

		# Increment counter
		let COUNTER++
		if [ ${COUNTER} -ge ${DATABRIDGE_BULK_SIZE} ]; then
			log 1 "Reached bulk limit of ${DATABRIDGE_BULK_SIZE} in download"
			break
		fi

	done

	# Expire and process rotation file
	while read JOB; do

		# Skip empty lines
		[ -z "$JOB" ] && continue

		# The entry without ':' comes first. If found and the next entry has the same ID
		# it means that this ID is processed and should not be accounted in the rotated file
		if [[ $JOB != *":"* ]]; then 
			CHECK_ID="$JOB"
		else

			# Get job details
			JOB_ID=$(echo "$JOB" | awk -F':' '{print $1}')
			JOB_TS=$(echo "$JOB" | awk -F':' '{print $2}')

			# Matching JOB_ID <-> CHECK_ID means that the job is processed,
			# so just ignore such jobs
			if [ "$JOB_ID" != "$CHECK_ID" ]; then

				# Check if the job is expired expired job
				let DELTA=$(date +%s)-${JOB_TS}
				if [[ ! -z "${DATABRIDGE_JOB_TIMEOUT}" && ${DELTA} -gt ${DATABRIDGE_JOB_TIMEOUT} ]]; then
					# Warn the user, and don't touch stale counter
					log 2 "Job ${JOB_ID} expired after ${DELTA} seconds"

					# Fire appropriate callback
					if [ ! -z "${DATABRIDGE_CB_EXPIRED}" ]; then
						# Fire callback
						${DATABRIDGE_CB_EXPIRED} "${JOB_ID}"
						ANS=$?
						# Check result
						if [ $? -ne 0 ]; then
							log 2 "EXPIRED callback failed (exit code $ANS)"
						else
							log 1 "EXPIRED callback executed successfully"
						fi
					fi

				else
					# Otherwise process later
					echo "$JOB" >> ${ROTATE_FILE}
				fi
			fi

		fi
		
	done < <(sort ${ROTATE_TMP})

	# Rotate state file
	cp -f --no-preserve=mode,ownership ${ROTATE_FILE} ${DATABRIDGE_JOB_LIST}

}

function download_jobs_list {
	local DELTA=0
	local JOB=""
	local JOB_ID=""
	local JOB_TS=""
	local CHECK_ID=""
	local LIST_SERVER="${TMP_DIR}/incoming.list"
	local LIST_SERVERIDS="${TMP_DIR}/ids.server"
	local LIST_LOCALIDS="${TMP_DIR}/ids.local"
	local ROTATE_TMP="${TMP_DIR}/joblist.tmp"
	local ROTATE_FILE="${TMP_DIR}/joblist.rotate"

	# Download the list of jobs in the queue
	log 1 "Enumerating files in the server output directory"
	curl -f -X GET --key ${DATABRIDGE_SSL_KEY} --cert ${DATABRIDGE_SSL_CERT} -k -L -s "${DATABRIDGE_OUTPUT_URL}" | \
		grep 'metalink"><a href="' | \
		sed -r 's/.*metalink"><a href="([^"?]+).*/\1/' > ${LIST_SERVER}

	# Check if list is empty
	if [ ! -s "${LIST_SERVER}" ]; then
		log 2 "Server output directory is empty. Suspicious..."
		return
	fi

	# Get a sorted list of two IDs
	cat "${LIST_SERVER}" | grep .tgz | sed 's/.tgz//' | sort > ${LIST_SERVERIDS}
	cat "${DATABRIDGE_JOB_LIST}" | awk -F':' '{print $1}' | sort > ${LIST_LOCALIDS}

	# Find IDs present in both our local and remote lists 
	# which effectively menans that a job we are monitoring has
	# placed it's output on the server.
	while read JOB_ID; do

		# Try to get job file
		get_outfile ${JOB_ID}

	done < <(comm -12 ${LIST_SERVERIDS} ${LIST_LOCALIDS})

	# Process all jobs in server that are not in our local list,
	# which effectively means jobs that are not trackable any longer
	# and should be safely disposed.
	# while read JOB_ID; do

	# 	# Delete job
	# 	log 1 "Deleting garbage data of job ${JOB_ID}"
	# 	curl -f -X DELETE --key ${DATABRIDGE_SSL_KEY} --cert ${DATABRIDGE_SSL_CERT} -k -L -s "${DATABRIDGE_OUTPUT_URL}/${JOB_ID}.tgz"
	# 	if [ $? -ne 0 ]; then
	# 		log 2 "Could not delete garbage of job ${JOB_ID}"
	# 	fi

	# done < <(comm -23 ${LIST_SERVERIDS} ${LIST_LOCALIDS})

	# Find all the IDs of pending jobs in the queue.
	# Include in the same file the job database in order to 
	# easily identify which entries are present (after sorting).
	cp -f ${DATABRIDGE_JOB_LIST} ${ROTATE_TMP}
	comm -13 ${LIST_SERVERIDS} ${LIST_LOCALIDS} >> ${ROTATE_TMP}

	# Reset rotate file
	touch ${ROTATE_FILE}

	# Process all jobs in local queue that are not present in the server,
	# which effectively means jobs that we are waiting for their completion.
	while read JOB; do

		# Skip empty lines
		[ -z "$JOB" ] && continue

		# The entry without ':' comes first. If found and the next entry has the same ID
		# it means that this ID is pending completion
		if [[ $JOB != *":"* ]]; then 
			# Keep the job ID
			CHECK_ID="$JOB"
		else
			# Get job details
			JOB_ID=$(echo "$JOB" | awk -F':' '{print $1}')
			JOB_TS=$(echo "$JOB" | awk -F':' '{print $2}')

			# Check if we should keep this for further processing
			if [ "$JOB_ID" == "$CHECK_ID" ]; then

				# Check for expired job
				let DELTA=$(date +%s)-${JOB_TS}
				if [[ ! -z "${DATABRIDGE_JOB_TIMEOUT}" && ${DELTA} -gt ${DATABRIDGE_JOB_TIMEOUT} ]]; then
					# Warn the user, and don't touch stale counter
					log 2 "Job ${JOB_ID} expired after ${DELTA} seconds"

					# Fire appropriate callback
					if [ ! -z "${DATABRIDGE_CB_EXPIRED}" ]; then
						# Fire callback
						${DATABRIDGE_CB_EXPIRED} "${JOB_ID}"
						ANS=$?
						# Check result
						if [ $? -ne 0 ]; then
							log 2 "EXPIRED callback failed (exit code $ANS)"
						else
							log 1 "EXPIRED callback executed successfully"
						fi
					fi

				else
					# Otherwise process later
					echo "$JOB" >> ${ROTATE_FILE}
				fi
			fi

		fi
		
	done < <(sort ${ROTATE_TMP})

	# Rotate state file
	cp -f --no-preserve=mode,ownership ${ROTATE_FILE} ${DATABRIDGE_JOB_LIST}

}

function download_jobs_sequential {
	local STALE_COUNTER=0
	local JOB_ID=""
	local JOB_TS=""
	local JOB=""
	local ANS=0
	local ROTATE_FILE="${TMP_DIR}/joblist.rotate"

	# Prepare rotation file
	echo -n "" > ${ROTATE_FILE}

	# Start reading the state file
	while read JOB; do

		# Skip empty lines
		[ -z "$JOB" ] && continue

		# Drain staled state
		if [ $STALE_COUNTER -gt ${DATABRIDGE_DOWNLOAD_STALE_RATE} ]; then
			echo "$JOB" >> ${ROTATE_FILE}
			continue
		fi

		# Get job info
		JOB_ID=$(echo "$JOB" | awk -F':' '{print $1}')
		JOB_TS=$(echo "$JOB" | awk -F':' '{print $2}')

		# Try to get job file
		get_outfile ${JOB_ID}

		# If we didn't manage, re-schedule attempt later
		if [ $? -ne 0 ]; then

			# Check for expired job
			let DELTA=$(date +%s)-${JOB_TS}
			if [[ ! -z "${DATABRIDGE_JOB_TIMEOUT}" && ${DELTA} -gt ${DATABRIDGE_JOB_TIMEOUT} ]]; then
				# Warn the user, and don't touch stale counter
				log 2 "Job ${JOB_ID} expired after ${DELTA} seconds"

				# Fire appropriate callback
				if [ ! -z "${DATABRIDGE_CB_EXPIRED}" ]; then
					# Fire callback
					${DATABRIDGE_CB_EXPIRED} "${JOB_ID}"
					ANS=$?
					# Check result
					if [ $? -ne 0 ]; then
						log 2 "EXPIRED callback failed (exit code $ANS)"
					else
						log 1 "EXPIRED callback executed successfully"
					fi
				fi

			else
				# Otherwise process later
				echo "$JOB" >> ${ROTATE_FILE}
				# Increment stale counter
				let STALE_COUNTER+=1
				# Warn for stale
				[  $STALE_COUNTER -gt ${DATABRIDGE_DOWNLOAD_STALE_RATE} ] && log 2 "Too many input jobs without data. Reading staled!"
			fi

		else

			# Job downloaded
			log 1 "Job ${JOB_ID} completed"

			# Reset stale counter
			STALE_COUNTER=0

		fi

	done < ${DATABRIDGE_JOB_LIST}

	# Rotate state file
	cp -f --no-preserve=mode,ownership ${ROTATE_FILE} ${DATABRIDGE_JOB_LIST}

}

# Check for override in the config file
[ ! -z "$1" ] && CONFIG_FILE=$1

# Lookup for the config
if [ ! -f ${CONFIG_FILE} ]; then
	log 3 "Could not find databridge server configuration in ${CONFIG_FILE}!"
	exit 1
fi

# Source config
. ${CONFIG_FILE}

# Multiple instances trap
PIDFILE=/var/run/databridge-server-sync.pid
if [ -f "$PIDFILE" ] && kill -0 `cat $PIDFILE` 2>/dev/null; then
	log 3 "Another instance is already running"
	exit 1
fi  
echo $$ > $PIDFILE

# Make sure we have input/output directories
mkdir -p ${DATABRIDGE_INPUT_DIR}
mkdir -p ${DATABRIDGE_OUTPUT_DIR}
mkdir -p $(dirname ${DATABRIDGE_JOB_LIST})

# Create a temporary directory
TMP_DIR=$(mktemp -d)

# Start by downloading jobs
if [ "${DATABRIDGE_PROBE_TYPE}" == "1" ]; then
	# Use the list algorithm
	log 1 "Downloading job outputs from DataBridge using job listing"
	download_jobs_list
elif [ "${DATABRIDGE_PROBE_TYPE}" == "2" ]; then
	# Use the fifo algorithm
	log 1 "Downloading job outputs from DataBridge using output queue"
	download_jobs_fifo
else
	# Use sequential probing algorithm
	log 1 "Downloading job outputs from DataBridge using sequential probing"
	download_jobs_sequential
fi

# Then upload jobs
log 1 "Uploading new jobs to DataBridge"
upload_jobs

# Remove temporary directory
rm -rf "${TMP_DIR}"

# We are done
log 1 "DataBridge synchronization completed"
