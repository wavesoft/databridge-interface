#!/bin/bash
#
# DataBridge Server Synchronization Script for simple projects
# Copyright (C) 2014-2015  Ioannis Charalampidis, PH-SFT, CERN

# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
#

# Where the server configuration of DataBridge server-sync is located
CONFIG_FILE="/etc/databridge/server.conf"

# Minimum log level
LOG_LEVEL=1

function log {
	local LEVEL=$1
	local LEVEL_STR="DEBUG"
	shift

	# Do not log levels lower than
	# the ones specified
	[ $LEVEL -lt $LOG_LEVEL ] && return

	# Translate level to string
	if [ $LEVEL -eq 1 ]; then
		LEVEL_STR="INFO"
	elif [ $LEVEL -eq 2 ]; then
		LEVEL_STR="WARN"
	elif [ $LEVEL -eq 3 ]; then
		LEVEL_STR="ERROR"
	fi

	# Check for log destination
	if [ -z "${DATABRIDGE_LOG_FILE}" ]; then
	    # Process multiline buffers
	    while read L; do
	    	[ -z "$L" ] && continue
	        echo "[$(date '+%d/%m/%Y %H:%M:%S')] ${LEVEL_STR}: $L" 1>&2
	    done <<< "$*"
	else
	    # Process multiline buffers
	    while read L; do
	    	[ -z "$L" ] && continue
	        echo "[$(date '+%d/%m/%Y %H:%M:%S')] ${LEVEL_STR}: $L" >> ${DATABRIDGE_LOG_FILE}
	    done <<< "$*"
	fi
}

# DAVIX I/O Helper (file-based)
function davix_io {
    local VERB=$1
    local URL=$2
    local FILE=$3
    local RET=256
    local ERROR_MSG=""
	local DAVIX_LOG="${TMP_DIR}/davix-debug.log"
    local RETRIES=3
    shift; shift; shift

    # Extra args
    local CMDLINE="--retry 1 -k --key \"${DATABRIDGE_SSL_KEY}\" --cert \"${DATABRIDGE_SSL_CERT}\""
    for (( i = 1; i <= $# ; i++ )); do
        eval ARG=\$$i
        if [[ $ARG =~ $WHITESPACE ]]
        then
            CMDLINE="$CMDLINE \"$ARG\""
        else
            CMDLINE="$CMDLINE $ARG"
        fi
    done

    # Check if we should enable davix debugging
    if [[ $LOG_LEVEL -le 0 ]]; then
    	CMDLINE="$CMDLINE --debug"
    fi

    # Handle verb accordint to case
    case $VERB in
        GET)
            # Use davix-get to download a file
            CMDLINE="davix-get $CMDLINE \"${URL}\" ${FILE}"
            ;;
        PUT)
            # Use davix-put to upload a file
            CMDLINE="davix-put $CMDLINE ${FILE} \"${URL}\""
            ;;
        DELETE)
            # Use davix-rm to delete a file
            CMDLINE="davix-rm $CMDLINE \"${URL}\""
            ;;
        *)
            log 3 "Unhandled DAVIX Verb '${VERB}'"
            return 200
            ;;
    esac

    # Repeat command if it fails
    while true; do

	    # Perform DAVOX I/O
	    log 0 "Executing $CMDLINE"
	    exec 3>&1
	    ERROR_MSG=$(eval $CMDLINE 2>&1)
	    RET=$?
	    exec 3>&-
	    log 0 "Exited with $RET"

	    # If this was an error, retry
	    if [[ $RET -ne 0 && $RETRIES -gt 0 ]]; then
	    	log 3 "$ERROR_MSG"

            # There is no need to retry on 404 errors
            if [[ $ERROR_MSG == *"404"* ]]; then
                return 244
            fi

	    	let RETRIES--
	    	log 1 "In 10 sec will retry a DAVIX ${VERB} to ${URL}"
	    	sleep 10
	    else
	    	break
	    fi

    done

    # Log Errors
    [ $RET -ne 0 ] && log 3 "$ERROR_MSG"

    # Return exit code
    return $RET
}

# CURL I/O Helper (payload-based)
function curl_io {
    local VERB=$1
    local URL=$2
    local RET=0
    local ERROR_MSG=""
    local WHITESPACE="[[:space:]]"
    shift; shift

    # Extra args
    local CMDLINE="curl -k --key \"${DATABRIDGE_SSL_KEY}\" --cert \"${DATABRIDGE_SSL_CERT}\""
    CMDLINE="${CMDLINE} -f -s -S -L --retry 3 --retry-delay 10 -X ${VERB}"
    for (( i = 1; i <= $# ; i++ )); do
        eval ARG=\$$i
        if [[ $ARG =~ $WHITESPACE ]]
        then
            CMDLINE="$CMDLINE \"$ARG\""
        else
            CMDLINE="$CMDLINE $ARG"
        fi
    done
    CMDLINE="${CMDLINE} \"${URL}\""

    # Perform HTTP GET
    log 0 "Executing $CMDLINE"
    exec 3>&1
    ERROR_MSG=$(eval $CMDLINE 2>&1 1>&3)
    RET=$?
    exec 3>&-

    # Log Errors
    [ $RET -ne 0 ] && log 3 "$ERROR_MSG"

    # Return exit code
    return $RET
}

function put_jobfile {
	local FILE=$1
	local JOB_FILE="${TMP_DIR}/job.id"

	# Validate file
	[ ! -f ${FILE} ] && log 3 "File $FILE does not exist" && return 1

	# Get file UUID
	JOB_ID=$(uuidgen)	

	# Upload file on input queue
	log 1 "Uploading job file ${FILE}"
	davix_io PUT "${DATABRIDGE_INPUT_URL}/${JOB_ID}" "${FILE}"
	[ $? -ne 0 ] && return 1
	
	# Upload job ID on FIFO	
	log 1 "Uploading job description"
	curl_io PUT "${DATABRIDGE_QUEUE_URL}" -F "userdata=${JOB_ID}" --header "jobID: ${JOB_ID}"
	if [ $? -ne 0 ]; then

		# Remove from queue
		log 3 "Could not upload job description, removing job file"
		davix_io DELETE "${DATABRIDGE_INPUT_URL}/${JOB_ID}"
		[ $? -ne 0 ] && log 3 "Unable to delete input file: ${DATABRIDGE_INPUT_URL}/${JOB_ID}"

		# Return error
		return 1
	fi
	
	# Successfuly placed, update state file
	log 1 "Job ${JOB_ID} placed in queue"
	echo "$JOB_ID:$(date +%s)" >> $DATABRIDGE_JOB_LIST

	# Fire appropriate callback
	if [ ! -z "${DATABRIDGE_CB_SCHEDULED}" ]; then
		# Fire callback
		${DATABRIDGE_CB_SCHEDULED} "${FILE}" "${JOB_ID}"
		ANS=$?
		# Check result
		if [ $? -ne 0 ]; then
			log 2 "SCHEDULED callback failed (exit code $ANS)"
		else
			log 1 "SCHEDULED callback executed successfully"
		fi
	fi

	# Return 0
	return 0
}

function get_outfile {
	local JOB_ID=$1
	local H_LAST_MODIFIED=""
	local JOB_DATE=""
	local JOB_OUTPUT_DIR=""
	local F_JOB="${TMP_DIR}/jobout${DATABRIDGE_OUTPUT_EXT}"

	# Delete input file one way or another
	log 1 "Deleting input data of job ${JOB_ID}"
	davix_io DELETE "${DATABRIDGE_INPUT_URL}/${JOB_ID}"
	[ $? -ne 0 ] && log 2 "Unable to clean job input ${DATABRIDGE_INPUT_URL}/${JOB_ID}"

	# Try to get job file from output directory into the temporary files
	log 1 "Downloading job ${JOB_ID} from server"
	davix_io GET "${DATABRIDGE_OUTPUT_URL}/${JOB_ID}.tgz" "${F_JOB}"
	[ $? -ne 0 ] && return 1

	# Make sure directory exists
	JOB_OUTPUT_DIR="${DATABRIDGE_OUTPUT_DIR}/$(date +%Y%m%d)"
	if [ ! -d "${JOB_OUTPUT_DIR}" ]; then

		# Create directory
		mkdir -p "${JOB_OUTPUT_DIR}" 2>/dev/null
		[ $? -ne 0 ] && log 3 "Could not create directory ${JOB_OUTPUT_DIR}" && return 1

		# Change owner if reuqired
		if [ ! -z "${DATABRIDGE_OUTPUT_OWNER}" ]; then
			chown ${DATABRIDGE_OUTPUT_OWNER} "${JOB_OUTPUT_DIR}" 2>/dev/null
			[ $? -ne 0 ] && log 2 "Could not change ownership of ${JOB_OUTPUT_DIR}"
		fi

	fi

	# Move file to the proper location
	JOB_OUTFILE="${JOB_OUTPUT_DIR}/${JOB_ID}${DATABRIDGE_OUTPUT_EXT}"
	mv "${F_JOB}" "${JOB_OUTFILE}" 2>/dev/null
	[ $? -ne 0 ] && log 3 "Could not move job file to ${JOB_OUTPUT_DIR}/${JOB_ID}${DATABRIDGE_OUTPUT_EXT}" && return 1

	# Change owner if reuqired
	if [ ! -z "${DATABRIDGE_OUTPUT_OWNER}" ]; then
		chown ${DATABRIDGE_OUTPUT_OWNER} "${JOB_OUTPUT_DIR}/${JOB_ID}${DATABRIDGE_OUTPUT_EXT}" 2>/dev/null
		[ $? -ne 0 ] && log 2 "Could not change ownership of ${JOB_OUTPUT_DIR}/${JOB_ID}${DATABRIDGE_OUTPUT_EXT}"
	fi

	# Everything worked as expected, delete output (cleanup)
	davix_io DELETE "${DATABRIDGE_OUTPUT_URL}/${JOB_ID}.tgz"
	[ $? -ne 0 ] && log 2 "Unable to clean job output ${DATABRIDGE_OUTPUT_URL}/${JOB_ID}.tgz"

	# Fire appropriate callback
	if [ ! -z "${DATABRIDGE_CB_COMPLETED}" ]; then
		# Fire callback
		${DATABRIDGE_CB_COMPLETED} "${JOB_OUTFILE}" "${JOB_ID}"
		ANS=$?
		# Check result
		if [ $? -ne 0 ]; then
			log 2 "COMPLETED callback failed (exit code $ANS)"
		else
			log 1 "COMPLETED callback executed successfully"
		fi
	fi

	# Return 0
	return 0
}

function upload_jobs {
	local COUNTER=0
	local FILE=""
	local QUEUE_SIZE=0
	local UPLOAD_LIMIT=${DATABRIDGE_BULK_SIZE}

	# Check if we should cap the UPLOAD_LIMIT
	if [ ${DATABRIDGE_MAX_QUEUE_SIZE} -gt 0 ]; then

		# Check the length of the queue
		QUEUE_SIZE=$(cat ${DATABRIDGE_JOB_LIST} | wc -l)

		# Check delta
		let UPLOAD_LIMIT=${DATABRIDGE_MAX_QUEUE_SIZE}-${QUEUE_SIZE}

		# If less than 0, don't bother
		if [ ${UPLOAD_LIMIT} -le 0 ]; then
			log 1 "Skipping because queue contains ${QUEUE_SIZE} jobs"
			return
		fi

	fi

	# Start submitting jobs
	for FILE in ${DATABRIDGE_INPUT_DIR}/*; do

		# Put job file
		put_jobfile $FILE

		# Upon successful placement, remove
		if [ $? -eq 0 ]; then

			# Delete file
			rm $FILE

		else
			log 3 "Could not upload file $FILE, will try later"
		fi

		# Increment counter
		let COUNTER+=1
		if [ $COUNTER -ge ${UPLOAD_LIMIT} ]; then
			log 1 "Reached limit of ${UPLOAD_LIMIT} jobs per submission"
			break
		fi

	done

}

function download_jobs_fifo {
	local DELTA=0
	local COUNTER=0
	local JOB=""
	local JOB_ID=""
	local JOB_TS=""
	local CHECK_ID=""
	local ROTATE_TMP="${TMP_DIR}/joblist.tmp"
	local ROTATE_FILE="${TMP_DIR}/joblist.rotate"

	# Copy the IDs of the jobs pending in the queue.
	# This will help us at a later time to track changes.
	cp -f ${DATABRIDGE_JOB_LIST} ${ROTATE_TMP}

	# Make sure we have a rotate file
	touch ${ROTATE_FILE}

	# Repeat for a while
	while true; do

		# Get next job from the output fifo
		JOB_ID=$(curl_io GET "${DATABRIDGE_OUTQUEUE_URL}")
		if [ $? -ne 0 ]; then
			log 3 "DataBridge Output Queue Error"
			break
		fi

		# Try to get job file
		get_outfile ${JOB_ID}
		if [ $? -eq 0 ]; then
			log 1 "Job ${JOB_ID} completed"
		else
			log 1 "Job ${JOB_ID} could not be downloaded"
		fi

		# Keep the ID of the processed job
		echo "${JOB_ID}" >> ${ROTATE_TMP}

		# Increment counter
		let COUNTER++
		if [ ${COUNTER} -ge ${DATABRIDGE_BULK_SIZE} ]; then
			log 1 "Reached bulk limit of ${DATABRIDGE_BULK_SIZE} in download"
			break
		fi

	done

	# Expire and process rotation file
	while read JOB; do

		# Skip empty lines
		[ -z "$JOB" ] && continue

		# The entry without ':' comes first. If found and the next entry has the same ID
		# it means that this ID is processed and should not be accounted in the rotated file
		if [[ $JOB != *":"* ]]; then 
			CHECK_ID="$JOB"
		else

			# Get job details
			JOB_ID=$(echo "$JOB" | awk -F':' '{print $1}')
			JOB_TS=$(echo "$JOB" | awk -F':' '{print $2}')

			# Matching JOB_ID <-> CHECK_ID means that the job is processed,
			# so just ignore such jobs
			if [ "$JOB_ID" != "$CHECK_ID" ]; then

				# Check if the job is expired expired job
				let DELTA=$(date +%s)-${JOB_TS}
				if [[ ${DATABRIDGE_JOB_TIMEOUT} -gt 0 && ${DELTA} -gt ${DATABRIDGE_JOB_TIMEOUT} ]]; then
					# Warn the user, and don't touch stale counter
					log 2 "Job ${JOB_ID} expired after ${DELTA} seconds"

					# Check if we should not re-schedule
					if [ ${DATABRIDGE_EXPIRE_RESCHEDULE} -eq 0 ]; then

						# Fire appropriate callback
						if [ ! -z "${DATABRIDGE_CB_EXPIRED}" ]; then
							# Fire callback
							${DATABRIDGE_CB_EXPIRED} "${JOB_ID}"
							ANS=$?
							# Check result
							if [ $? -ne 0 ]; then
								log 2 "EXPIRED callback failed (exit code $ANS)"
							else
								log 1 "EXPIRED callback executed successfully"
							fi
						fi

						# Delete input files
						davix_io DELETE "${DATABRIDGE_INPUT_URL}/${JOB_ID}"
						[ $? -ne 0 ] && log 3 "Unable to delete input file: ${DATABRIDGE_INPUT_URL}/${JOB_ID}"

					# Otherwise re-schedule
					else

						# Upload job ID on FIFO	
						log 1 "Putting job ${JOB_ID} description back in queue"
						curl_io PUT "${DATABRIDGE_QUEUE_URL}" -F "userdata=${JOB_ID}" --header "jobID: ${JOB_ID}"
						if [ $? -ne 0 ]; then
							# Remove from queue
							log 3 "Could not upload job description, removing job file"
							davix_io DELETE "${DATABRIDGE_INPUT_URL}/${JOB_ID}"
							[ $? -ne 0 ] && log 3 "Unable to delete input file: ${DATABRIDGE_INPUT_URL}/${JOB_ID}"
						else
							# Re-schedule job
							echo "$JOB_ID:$(date +%s)" >> ${ROTATE_FILE}	
						fi

					fi


				else
					# Otherwise process later
					echo "$JOB" >> ${ROTATE_FILE}
				fi
			fi

		fi
		
	done < <(sort ${ROTATE_TMP})

	# Rotate state file
	cp -f --no-preserve=mode,ownership ${ROTATE_FILE} ${DATABRIDGE_JOB_LIST}

}

function download_jobs_list {
	local DELTA=0
	local JOB=""
	local JOB_ID=""
	local JOB_TS=""
	local CHECK_ID=""
	local LIST_SERVER="${TMP_DIR}/incoming.list"
	local LIST_SERVERIDS="${TMP_DIR}/ids.server"
	local LIST_LOCALIDS="${TMP_DIR}/ids.local"
	local ROTATE_TMP="${TMP_DIR}/joblist.tmp"
	local ROTATE_FILE="${TMP_DIR}/joblist.rotate"

	# Download the list of jobs in the queue
	log 1 "Enumerating files in the server output directory"
	curl_io GET "${DATABRIDGE_OUTPUT_URL}" | \
		grep 'metalink"><a href="' | \
		sed -r 's/.*metalink"><a href="([^"?]+).*/\1/' > ${LIST_SERVER}

	# On CURL errors, log error
	if [ ${PIPESTATUS[0]} -ne 0 ]; then
		log 3 "Unable to enumerate files in the output directory"
		return 1
	fi

	# Check if list is empty
	if [ ! -s "${LIST_SERVER}" ]; then
		log 2 "Server output directory is empty. Suspicious..."
		return
	fi

	# Get a sorted list of two IDs
	cat "${LIST_SERVER}" | grep .tgz | sed 's/.tgz//' | sort > ${LIST_SERVERIDS}
	cat "${DATABRIDGE_JOB_LIST}" | awk -F':' '{print $1}' | sort > ${LIST_LOCALIDS}

	# Find IDs present in both our local and remote lists 
	# which effectively menans that a job we are monitoring has
	# placed it's output on the server.
	while read JOB_ID; do

		# Try to get job file
		get_outfile ${JOB_ID}

	done < <(comm -12 ${LIST_SERVERIDS} ${LIST_LOCALIDS})

	# Process all jobs in server that are not in our local list,
	# which effectively means jobs that are not trackable any longer
	# and should be safely disposed.
	# while read JOB_ID; do

	# 	# Delete job
	# 	log 1 "Deleting garbage data of job ${JOB_ID}"
	# 	curl -f -X DELETE --key ${DATABRIDGE_SSL_KEY} --cert ${DATABRIDGE_SSL_CERT} -k -L -s "${DATABRIDGE_OUTPUT_URL}/${JOB_ID}.tgz"
	# 	if [ $? -ne 0 ]; then
	# 		log 2 "Could not delete garbage of job ${JOB_ID}"
	# 	fi

	# done < <(comm -23 ${LIST_SERVERIDS} ${LIST_LOCALIDS})

	# Find all the IDs of pending jobs in the queue.
	# Include in the same file the job database in order to 
	# easily identify which entries are present (after sorting).
	cp -f ${DATABRIDGE_JOB_LIST} ${ROTATE_TMP}
	comm -13 ${LIST_SERVERIDS} ${LIST_LOCALIDS} >> ${ROTATE_TMP}

	# Reset rotate file
	touch ${ROTATE_FILE}

	# Process all jobs in local queue that are not present in the server,
	# which effectively means jobs that we are waiting for their completion.
	while read JOB; do

		# Skip empty lines
		[ -z "$JOB" ] && continue

		# The entry without ':' comes first. If found and the next entry has the same ID
		# it means that this ID is pending completion
		if [[ $JOB != *":"* ]]; then 
			# Keep the job ID
			CHECK_ID="$JOB"
		else
			# Get job details
			JOB_ID=$(echo "$JOB" | awk -F':' '{print $1}')
			JOB_TS=$(echo "$JOB" | awk -F':' '{print $2}')

			# Check if we should keep this for further processing
			if [ "$JOB_ID" == "$CHECK_ID" ]; then

				# Check for expired job
				let DELTA=$(date +%s)-${JOB_TS}
				if [[ ${DATABRIDGE_JOB_TIMEOUT} -gt 0 && ${DELTA} -gt ${DATABRIDGE_JOB_TIMEOUT} ]]; then
					# Warn the user, and don't touch stale counter
					log 2 "Job ${JOB_ID} expired after ${DELTA} seconds"

					# Fire appropriate callback
					if [ ! -z "${DATABRIDGE_CB_EXPIRED}" ]; then
						# Fire callback
						${DATABRIDGE_CB_EXPIRED} "${JOB_ID}"
						ANS=$?
						# Check result
						if [ $? -ne 0 ]; then
							log 2 "EXPIRED callback failed (exit code $ANS)"
						else
							log 1 "EXPIRED callback executed successfully"
						fi
					fi

				else
					# Otherwise process later
					echo "$JOB" >> ${ROTATE_FILE}
				fi
			fi

		fi
		
	done < <(sort ${ROTATE_TMP})

	# Rotate state file
	cp -f --no-preserve=mode,ownership ${ROTATE_FILE} ${DATABRIDGE_JOB_LIST}

}

function download_jobs_sequential {
	local STALE_COUNTER=0
	local JOB_ID=""
	local JOB_TS=""
	local JOB=""
	local ANS=0
	local ROTATE_FILE="${TMP_DIR}/joblist.rotate"

	# Prepare rotation file
	echo -n "" > ${ROTATE_FILE}

	# Start reading the state file
	while read JOB; do

		# Skip empty lines
		[ -z "$JOB" ] && continue

		# Drain staled state
		if [ $STALE_COUNTER -gt ${DATABRIDGE_DOWNLOAD_STALE_RATE} ]; then
			echo "$JOB" >> ${ROTATE_FILE}
			continue
		fi

		# Get job info
		JOB_ID=$(echo "$JOB" | awk -F':' '{print $1}')
		JOB_TS=$(echo "$JOB" | awk -F':' '{print $2}')

		# Try to get job file
		get_outfile ${JOB_ID}

		# If we didn't manage, re-schedule attempt later
		if [ $? -ne 0 ]; then

			# Check for expired job
			let DELTA=$(date +%s)-${JOB_TS}
			if [[ ${DATABRIDGE_JOB_TIMEOUT} -gt 0 && ${DELTA} -gt ${DATABRIDGE_JOB_TIMEOUT} ]]; then
				# Warn the user, and don't touch stale counter
				log 2 "Job ${JOB_ID} expired after ${DELTA} seconds"

				# Fire appropriate callback
				if [ ! -z "${DATABRIDGE_CB_EXPIRED}" ]; then
					# Fire callback
					${DATABRIDGE_CB_EXPIRED} "${JOB_ID}"
					ANS=$?
					# Check result
					if [ $? -ne 0 ]; then
						log 2 "EXPIRED callback failed (exit code $ANS)"
					else
						log 1 "EXPIRED callback executed successfully"
					fi
				fi

			else
				# Otherwise process later
				echo "$JOB" >> ${ROTATE_FILE}
				# Increment stale counter
				let STALE_COUNTER+=1
				# Warn for stale
				[  $STALE_COUNTER -gt ${DATABRIDGE_DOWNLOAD_STALE_RATE} ] && log 2 "Too many input jobs without data. Reading staled!"
			fi

		else

			# Job downloaded
			log 1 "Job ${JOB_ID} completed"

			# Reset stale counter
			STALE_COUNTER=0

		fi

	done < ${DATABRIDGE_JOB_LIST}

	# Rotate state file
	cp -f --no-preserve=mode,ownership ${ROTATE_FILE} ${DATABRIDGE_JOB_LIST}

}

# Check for override in the config file
[ ! -z "$1" ] && CONFIG_FILE=$1 && shift

# Lookup for the config
if [ ! -f ${CONFIG_FILE} ]; then
	log 3 "Could not find databridge server configuration in ${CONFIG_FILE}!"
	exit 1
fi

# Source config
. ${CONFIG_FILE}

# Set some defaults
[ -z "${DATABRIDGE_JOB_TIMEOUT}" ] && DATABRIDGE_JOB_TIMEOUT=0
[ -z "${DATABRIDGE_EXPIRE_RESCHEDULE}" ] && DATABRIDGE_EXPIRE_RESCHEDULE=1

# Override log level from the command line
for L in $@; do
	case $L in
		--debug) LOG_LEVEL=0
				 ;;
	esac
done

# Multiple instances trap
PIDFILE=/var/run/databridge-server-sync.pid
if [ -f "$PIDFILE" ] && kill -0 `cat $PIDFILE` 2>/dev/null; then
	log 3 "Another instance is already running"
	exit 1
fi  
echo $$ > $PIDFILE

# Make sure we have input/output directories
mkdir -p ${DATABRIDGE_INPUT_DIR}
mkdir -p ${DATABRIDGE_OUTPUT_DIR}
mkdir -p $(dirname ${DATABRIDGE_JOB_LIST})

# Create a temporary directory
TMP_DIR=$(mktemp -d)

# Start by downloading jobs
if [ "${DATABRIDGE_PROBE_TYPE}" == "1" ]; then
	# Use the list algorithm
	log 1 "Downloading job outputs from DataBridge using job listing"
	download_jobs_list
elif [ "${DATABRIDGE_PROBE_TYPE}" == "2" ]; then
	# Use the fifo algorithm
	log 1 "Downloading job outputs from DataBridge using output queue"
	download_jobs_fifo
else
	# Use sequential probing algorithm
	log 1 "Downloading job outputs from DataBridge using sequential probing"
	download_jobs_sequential
fi

# Then upload jobs
log 1 "Uploading new jobs to DataBridge"
upload_jobs

# Remove temporary directory
rm -rf "${TMP_DIR}"

# We are done
log 1 "DataBridge synchronization completed"
